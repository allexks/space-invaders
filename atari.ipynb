{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2wsuq_uaCwE"
      },
      "source": [
        "# Курсова работа по \"Дълбоко самообучение с Тензорфлоу\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "на Александър Игнатов, Ф№0MI3400082, 05.07.2022г."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Тема: „Приложение на метода Deep Q-Learning (DQN) за играта Space Invaders за ретро игрови конзоли Atari 2600.“"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Избран обект от Gym\n",
        "\n",
        "Обект на обучението е играта Space Invaders за Atari 2600.\n",
        "\n",
        "![game](https://www.gymlibrary.ml/_images/space_invaders.gif)\n",
        "\n",
        "Играчът контролира наземна ракета, която придвижва наляво или надясно, и от която може да стреля нагоре. Точки печели, когато унищожава противникови ракети и губи играта, когато те достигнат земята или го унищожат с изстрел. Целта е максимално натрупване на точки в рамките на три живота.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gym ни предоставя модел на играта с три различни възможни начина на наблюдение на състоянието и в три различни версии (v0, v4, v5). Използвана от проекта версия е v4, като са проведени експерименти с два различни типа пространство на състоянието (наблюдение на пикселите на екрана и наблюдение на RAM паметта на играта) с цел сравнение на двата подхода.\n",
        "V4 се характеризира с това, че по подразбиране всяко действие се задържа за 2, 3 или 4 кадъра от играта на случаен принцип. Това прави стохастичен иначе детерминистичния модел на играта."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "При варианта с наблюдение на пикселите, състоянието на играта е тензор с размерности 210x160x3 (височина х ширина х цвят):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(210, 160, 3)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = gym.make(\"SpaceInvaders-v4\")\n",
        "env.observation_space.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "При варианта с наблюдение на RAM паметта, състоянието е вектор с дължина 128, понеже играта има само 128 байта в RAM паметта си:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "envram = gym.make(\"SpaceInvaders-v4\", obs_type=\"ram\")\n",
        "envram.observation_space.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Възможните действия в играта на всеки кадър са 6 на брой:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEd_Sh4nZfQU",
        "outputId": "75386dc0-948c-4e6d-e406-671cc383f94c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.unwrapped.get_action_meanings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "При избор на случайно действие на всяка стъпка се постига резултат от около 150 точки в играта (долният експеримент изчислява средноаритметичния reward при 100 епизода при случайно избиране на действие на всяка стъпка):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvgmfMu-Z3HG",
        "outputId": "73ee0323-520b-4089-c102-f2f346f4d751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average reward: 155.75\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "EPISODES = 100\n",
        "scores = []\n",
        "\n",
        "for episode in range(1, EPISODES + 1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    \n",
        "    while not done:\n",
        "        action = random.choice(range(env.action_space.n))\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "    \n",
        "    scores.append(score)\n",
        "\n",
        "avg = np.mean(scores)\n",
        "print(f\"Average reward: {avg}\")\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTzMWfxFdDrS"
      },
      "source": [
        "## Избран метод за обучение\n",
        "\n",
        "Избраният метод за обучение е Deep Q-Learning (DQN) с Experience Replay и $\\epsilon$\u000f-greedy стратегия (https://arxiv.org/pdf/1312.5602.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Псевдокод на алгоритъма:\n",
        "1. Инициализация на replay memory $D$ с капацитет $N$\n",
        "2. Инициализация на Q-функцията със случайни тегла\n",
        "3. За $episode = 1...M$, направи:\n",
        "    * Инициализирай редица $ s_1 = \\{x_1\\} $\n",
        "    * За $ t = 1... T $, направи:\n",
        "        * С вероятност $\\epsilon$ избери случайно действие $a_t$, в противен случай избери $ a_t = argmax_a Q^*(s_t, a; \\theta) $\n",
        "        * Изпълни действието в емулатора и наблюдавай награда $r_t$ и състояние $x_{t+1}$\n",
        "        * Запази прехода $ (s_t, a_t, r_t, s_{t+1}) $ в $ D $\n",
        "        * Вземи произволно малко количество от преходи $ (s_j , a_j , r_j , s_{j+1}) $ от $ D $\n",
        "        * Присвои $$ y_j =\n",
        "            \\begin{cases}\n",
        "            r_j & \\text{за терминален } s_{j+1} \\\\\n",
        "            r_j + \\gamma \\max_{a'} Q^*(s_{j+1}, a'; \\theta) & \\text{за нетерминален } s_{j+1}\n",
        "            \\end{cases} $$\n",
        "        * Направи градиентно спускане по $ (y_j − Q(s_j, a_j; \\theta))^2 $:\n",
        "$$ \\nabla_{\\theta_i}L_i(\\theta_i) = E_{s,a \\sim p(·); s' \\sim \\epsilon} [(r + \\gamma\\max_a'Q(s', a'; \\theta_{i-1}) - Q(s, a, \\theta_i))\\nabla_{\\theta_i}Q(s, a; \\theta_i)] $$\n",
        "$$ L_i(\\theta_i) = E_{s,a \\sim p(·)}[(y_i - Q(s, a; \\theta_i))^2] $$\n",
        "$$ Q^*(s, a) = E_{s' \\sim \\epsilon}[r + \\gamma\\max_a'Q^*(s', a') \\mid s, a] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Реализация и ескперименти"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFF_4nRjdJOy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Convolution2D, Resizing, Rescaling, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.image import rgb_to_grayscale\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.utils import register_keras_serializable\n",
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0. Функция за създаване на агента\n",
        "\n",
        "Хиперпараметрите, които са използвани, са:\n",
        "* Капацитет на experience replay паметта: 1 000 000\n",
        "* Намаляне на $\\epsilon$: от 1.0 до 0.1 в хода на 10 000 стъпки\n",
        "* Брой стъпки преди започване на обучението: 1 000\n",
        "* $\\gamma = 0.99$\n",
        "* Размер на batch: 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "un0NM5Cee05L"
      },
      "outputs": [],
      "source": [
        "def build_agent(model, actions, window_size):\n",
        "    policy = LinearAnnealedPolicy(\n",
        "        EpsGreedyQPolicy(), \n",
        "        attr='eps', \n",
        "        value_max=1.0, \n",
        "        value_min=0.1, \n",
        "        value_test=0.2, \n",
        "        nb_steps=10000\n",
        "    )\n",
        "    memory = SequentialMemory(\n",
        "        limit=1000000, \n",
        "        window_length=window_size\n",
        "    )\n",
        "    dqn = DQNAgent(\n",
        "        model=model, \n",
        "        memory=memory, \n",
        "        policy=policy,\n",
        "        enable_dueling_network=True, \n",
        "        dueling_type='avg', \n",
        "        nb_actions=actions, \n",
        "        nb_steps_warmup=1000\n",
        "    )\n",
        "    return dqn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Наблюдение на пикселното изображение на играта\n",
        "\n",
        "Стандартно за тази цел е използването на конволюционни слоеве в невронната мрежа с няколко филтри. Бе експериментирано с различни конфигурации на мрежата, като при всички тях времето за обучение бе изключително бавно. Поради това бе взето решение изображението с размер 210х160 да се намаля наполовина по двете измерения и конвертира в черно-бяло такова. Това намаля размерността на входящия тензор до 105х80х1, което значително сваля бройката на параметрите за обучение, но не даде забележим резултат за времето за обучение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YWjHyJ6T3Bof"
      },
      "outputs": [],
      "source": [
        "@register_keras_serializable(\"atari\")\n",
        "class GrayscaleLayer(Layer):\n",
        "  def call(self, input):\n",
        "    return rgb_to_grayscale(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_QXMNUladM_T"
      },
      "outputs": [],
      "source": [
        "def build_model(window_size, height, width, channels, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(window_size, height, width, channels)))\n",
        "    model.add(Reshape((window_size * height, width, channels), name=\"reshape_stack\"))\n",
        "    model.add(GrayscaleLayer(name=\"grayscale\"))\n",
        "    model.add(Resizing((window_size * height) // 2, width // 2, name=\"resize_half\"))\n",
        "    model.add(Rescaling(1./255, name=\"normalize\")) # normalize to [0, 1]\n",
        "    model.add(Reshape((window_size, height // 2, width // 2, 1), name=\"reshape_unstack\"))\n",
        "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', name=\"conv1\"))\n",
        "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu', name=\"conv2\"))\n",
        "    model.add(Convolution2D(64, (3,3), activation='relu', name=\"conv3\"))\n",
        "    model.add(Flatten(name=\"flatten\"))\n",
        "    model.add(Dense(512, activation='relu', name=\"fully_connected_1\"))\n",
        "    # model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear', name=\"output\"))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6rdDUGF1dgVE"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 4\n",
        "height, width, channels = env.observation_space.shape\n",
        "actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Използвайки `WINDOW_SIZE = 4` задаваме едновременната обработка на последните 4 състояния на играта на всяка стъпка. Това се прави с цел определяне на посоката на разитие."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ_R6xoWdolT",
        "outputId": "4e6e1311-e274-4839-d9da-2327ddc268ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"seqmodel\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 840, 160, 3)       0         \n",
            "                                                                 \n",
            " grayscale (GrayscaleLayer)  (None, 840, 160, 1)       0         \n",
            "                                                                 \n",
            " resizing (Resizing)         (None, 420, 80, 1)        0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 420, 80, 1)        0         \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 4, 105, 80, 1)     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 4, 25, 19, 32)     2080      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 4, 11, 8, 64)      32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 9, 6, 64)       36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 13824)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               7078400   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,153,318\n",
            "Trainable params: 7,153,318\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model(WINDOW_SIZE, height, width, channels, actions)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Както се вижда, получената мрежа с три конволюционни слоя и един изцяло-свързан слой с 512 неврона съдържа малко над 7 милиона параметъра (тегла) за обучение, които дори и с видеокартите, предоставени от Google Collab са твърде много за да може обучението да се впише в предоставениете ми няколко часа прозорец за активност на платформата."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XCK4WyX1f1VF"
      },
      "outputs": [],
      "source": [
        "dqn = build_agent(model, actions, WINDOW_SIZE)\n",
        "dqn.compile(Adam(learning_rate=0.00025))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOP8OrohiUj6",
        "outputId": "12d7ef3f-9081-4010-f3c7-2fe14f902a02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 10000 steps ...\n",
            "done, took 14326.813 seconds\n"
          ]
        },
        {
          "data": {},
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dqn.fit(env, nb_steps=10000, visualize=False, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxnlZ2DdqMmO",
        "outputId": "65be622b-8feb-4b11-c899-fc4d6bef1281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 40.000, steps: 694\n",
            "Episode 2: reward: 230.000, steps: 826\n",
            "Episode 3: reward: 25.000, steps: 570\n",
            "Episode 4: reward: 60.000, steps: 792\n",
            "Episode 5: reward: 35.000, steps: 557\n",
            "Episode 6: reward: 20.000, steps: 661\n",
            "Episode 7: reward: 225.000, steps: 967\n",
            "Episode 8: reward: 115.000, steps: 844\n",
            "Episode 9: reward: 80.000, steps: 677\n",
            "Episode 10: reward: 85.000, steps: 545\n",
            "Episode 11: reward: 75.000, steps: 672\n",
            "Episode 12: reward: 90.000, steps: 1360\n",
            "Episode 13: reward: 50.000, steps: 375\n",
            "Episode 14: reward: 105.000, steps: 1188\n",
            "Episode 15: reward: 80.000, steps: 683\n",
            "Episode 16: reward: 15.000, steps: 497\n",
            "Episode 17: reward: 340.000, steps: 988\n",
            "Episode 18: reward: 10.000, steps: 734\n",
            "Episode 19: reward: 65.000, steps: 435\n",
            "Episode 20: reward: 20.000, steps: 407\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Average reward: 88.25"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = dqn.test(env, nb_episodes=20, visualize=False)\n",
        "avg = np.mean(scores.history[\"episode_reward\"])\n",
        "print(f\"Average reward: {avg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Виждаме, че 10 000 стъпки са отнели 14326.813 секунди (почти 4 часа) и дали среден резултат от 88.25 точки на епизод, който е двойно по-лош от този при играта със случайни действия."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Наблюдение на RAM паметта на играта\n",
        "\n",
        "Предимството на използването на паметта като пространство от състояния на тази ретро игра, вместо пикселното изображение, което човек вижда, е че тя е само 128 байта, което е едно доста по-лесно смилаемо число от гледна точка на машинното самообучение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8yq6U0oPk8t",
        "outputId": "02ed6f52-c6a7-4962-bbbb-a5d49c132877"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "envram.observation_space.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Това позволява за много по-проста структура на невронна мрежа, която е по-бърза за обучение и по-бърза за изпълнение. Един или два скрити слоя от неврони са достатъчни за тази цел."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "NE0cVmGINY84"
      },
      "outputs": [],
      "source": [
        "def build_ram_model(ram_size, actions, window_size):\n",
        "    model = Sequential(name=\"ram_model\")\n",
        "    model.add(Input(shape=(window_size, ram_size)))\n",
        "    model.add(Flatten(name=\"flatten\"))\n",
        "    model.add(Dense(512, activation=\"relu\", name=\"fc1\"))\n",
        "    model.add(Dense(128, activation=\"relu\", name=\"fc2\"))\n",
        "    model.add(Dense(actions, activation=\"linear\", name=\"output\"))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LfVsYVgPxsY",
        "outputId": "7bf85efa-411e-451e-b2ee-c6fa3e7d68ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"ram_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 512)               262656     \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 128)               65664     \n",
            "                                                                 \n",
            " output (Dense)              (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 329,094\n",
            "Trainable params: 329,094\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "WINDOW_SIZE = 4\n",
        "ram_model = build_ram_model(envram.observation_space.shape[0], envram.action_space.n, WINDOW_SIZE)\n",
        "ram_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Виждаме, че при използване на два слоя с по 512 и 128 неврона съответно, параметрите за обучение са сведени от милиони само до малко под 400 хиляди, което е значителна оптимизация."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA3HzQueQRIZ",
        "outputId": "d3505947-b37b-4959-cfda-cd90ba2e5692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 100000 steps ...\n",
            "done, took 913.856 seconds\n"
          ]
        },
        {
          "data": {},
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ram_dqn = build_agent(ram_model, envram.action_space.n, WINDOW_SIZE)\n",
        "ram_dqn.compile(Adam(learning_rate=0.0002))\n",
        "ram_dqn.fit(envram, nb_steps=100_000, visualize=False, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTQ_pbQLRtc0",
        "outputId": "3f7cd905-8cdf-4a25-bf28-abe79fb5c37a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 225.000, steps: 898\n",
            "Episode 2: reward: 485.000, steps: 1027\n",
            "Episode 3: reward: 380.000, steps: 961\n",
            "Episode 4: reward: 80.000, steps: 393\n",
            "Episode 5: reward: 210.000, steps: 670\n",
            "Episode 6: reward: 135.000, steps: 525\n",
            "Episode 7: reward: 230.000, steps: 939\n",
            "Episode 8: reward: 220.000, steps: 639\n",
            "Episode 9: reward: 355.000, steps: 916\n",
            "Episode 10: reward: 400.000, steps: 1018\n",
            "Episode 11: reward: 125.000, steps: 637\n",
            "Episode 12: reward: 155.000, steps: 582\n",
            "Episode 13: reward: 110.000, steps: 468\n",
            "Episode 14: reward: 415.000, steps: 1119\n",
            "Episode 15: reward: 110.000, steps: 563\n",
            "Episode 16: reward: 80.000, steps: 528\n",
            "Episode 17: reward: 135.000, steps: 468\n",
            "Episode 18: reward: 120.000, steps: 538\n",
            "Episode 19: reward: 275.000, steps: 788\n",
            "Episode 20: reward: 355.000, steps: 1350\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Average reward: 230.0"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = ram_dqn.test(envram, nb_episodes=20, visualize=False)\n",
        "avg = np.mean(scores.history[\"episode_reward\"])\n",
        "print(f\"Average reward: {avg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "В този случай след само 100 000 стъпки агентът се е научил да играе по начин, с който бележи среден резултат от 230 точки на епизод, което е по-добро от 150-те точки при случаен избор на действие."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Използвани публикации и литература\n",
        "* https://nihit.github.io/resources/spaceinvaders.pdf\n",
        "* https://arxiv.org/pdf/1312.5602.pdf"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "atari.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c4ca5fb62c4de4ad8975f8d11547f3039fc9d01585eebbf720b5eb5352efe545"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
