{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2wsuq_uaCwE"
      },
      "source": [
        "# Space Invaders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "8J9pbywRcrR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.9.1 gym keras-rl2 gym[atari]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hoe-hREJaEfO",
        "outputId": "6f547396-b5c8-4ba5-cf95-455edb044ac0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.9.1\n",
            "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 511.7 MB 5.0 kB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Collecting keras-rl2\n",
            "  Downloading keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 858 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.46.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (0.26.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.0.0)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.21.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (3.3.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (14.0.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.15.0)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (3.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (21.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.1.2)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (3.1.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 18.8 MB/s \n",
            "\u001b[?25hCollecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 40.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.9.1) (1.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.1) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.9.1) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.3.7)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (2022.5.18.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.1) (3.2.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (0.2.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow==2.9.1) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow, keras-rl2\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 keras-rl2-1.0.5 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROM instructions: https://github.com/openai/atari-py#roms"
      ],
      "metadata": {
        "id": "NPjXbngKZAX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m atari_py.import_roms roms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4iIN1BQZOGp",
        "outputId": "de7639fd-cae9-46b2-f989-898adc828220"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copying space_invaders.bin from roms/Space Invaders.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploration and baseline"
      ],
      "metadata": {
        "id": "gS96fFQKcf86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7B2KcFLmcn6e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"SpaceInvaders-v4\")\n",
        "print(env.observation_space.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y074RXzxc1v7",
        "outputId": "50efda34-08da-4a4a-a320-75303c1759f5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 160, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.unwrapped.get_action_meanings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEd_Sh4nZfQU",
        "outputId": "c7b7c635-e593-4ec0-f078-7fd050576ca8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import clip\n",
        "\n",
        "EPISODES = 100\n",
        "scores = []\n",
        "\n",
        "for episode in range(1, EPISODES + 1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    \n",
        "    while not done:\n",
        "        # env.render()\n",
        "        action = random.choice(range(env.action_space.n))\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score += clip(reward, -1.0, 1.0)\n",
        "    \n",
        "    scores.append(score)\n",
        "    print(f\"Episode {episode}: Reward == {score}\")\n",
        "\n",
        "avg = np.mean(scores)\n",
        "print(f\"Average reward: {avg}\")\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvgmfMu-Z3HG",
        "outputId": "c4bffffb-1360-458a-e490-37fab460fdf5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Reward == 10.0\n",
            "Episode 2: Reward == 9.0\n",
            "Episode 3: Reward == 7.0\n",
            "Episode 4: Reward == 9.0\n",
            "Episode 5: Reward == 8.0\n",
            "Episode 6: Reward == 6.0\n",
            "Episode 7: Reward == 10.0\n",
            "Episode 8: Reward == 4.0\n",
            "Episode 9: Reward == 9.0\n",
            "Episode 10: Reward == 6.0\n",
            "Episode 11: Reward == 13.0\n",
            "Episode 12: Reward == 11.0\n",
            "Episode 13: Reward == 10.0\n",
            "Episode 14: Reward == 12.0\n",
            "Episode 15: Reward == 12.0\n",
            "Episode 16: Reward == 5.0\n",
            "Episode 17: Reward == 14.0\n",
            "Episode 18: Reward == 9.0\n",
            "Episode 19: Reward == 5.0\n",
            "Episode 20: Reward == 12.0\n",
            "Episode 21: Reward == 9.0\n",
            "Episode 22: Reward == 9.0\n",
            "Episode 23: Reward == 12.0\n",
            "Episode 24: Reward == 10.0\n",
            "Episode 25: Reward == 5.0\n",
            "Episode 26: Reward == 5.0\n",
            "Episode 27: Reward == 4.0\n",
            "Episode 28: Reward == 25.0\n",
            "Episode 29: Reward == 19.0\n",
            "Episode 30: Reward == 12.0\n",
            "Episode 31: Reward == 24.0\n",
            "Episode 32: Reward == 12.0\n",
            "Episode 33: Reward == 9.0\n",
            "Episode 34: Reward == 12.0\n",
            "Episode 35: Reward == 10.0\n",
            "Episode 36: Reward == 10.0\n",
            "Episode 37: Reward == 9.0\n",
            "Episode 38: Reward == 9.0\n",
            "Episode 39: Reward == 6.0\n",
            "Episode 40: Reward == 12.0\n",
            "Episode 41: Reward == 4.0\n",
            "Episode 42: Reward == 18.0\n",
            "Episode 43: Reward == 11.0\n",
            "Episode 44: Reward == 11.0\n",
            "Episode 45: Reward == 10.0\n",
            "Episode 46: Reward == 12.0\n",
            "Episode 47: Reward == 7.0\n",
            "Episode 48: Reward == 9.0\n",
            "Episode 49: Reward == 8.0\n",
            "Episode 50: Reward == 12.0\n",
            "Episode 51: Reward == 6.0\n",
            "Episode 52: Reward == 8.0\n",
            "Episode 53: Reward == 7.0\n",
            "Episode 54: Reward == 15.0\n",
            "Episode 55: Reward == 19.0\n",
            "Episode 56: Reward == 8.0\n",
            "Episode 57: Reward == 17.0\n",
            "Episode 58: Reward == 8.0\n",
            "Episode 59: Reward == 7.0\n",
            "Episode 60: Reward == 7.0\n",
            "Episode 61: Reward == 10.0\n",
            "Episode 62: Reward == 12.0\n",
            "Episode 63: Reward == 6.0\n",
            "Episode 64: Reward == 9.0\n",
            "Episode 65: Reward == 8.0\n",
            "Episode 66: Reward == 6.0\n",
            "Episode 67: Reward == 12.0\n",
            "Episode 68: Reward == 6.0\n",
            "Episode 69: Reward == 5.0\n",
            "Episode 70: Reward == 9.0\n",
            "Episode 71: Reward == 9.0\n",
            "Episode 72: Reward == 3.0\n",
            "Episode 73: Reward == 24.0\n",
            "Episode 74: Reward == 4.0\n",
            "Episode 75: Reward == 15.0\n",
            "Episode 76: Reward == 12.0\n",
            "Episode 77: Reward == 2.0\n",
            "Episode 78: Reward == 10.0\n",
            "Episode 79: Reward == 14.0\n",
            "Episode 80: Reward == 10.0\n",
            "Episode 81: Reward == 5.0\n",
            "Episode 82: Reward == 8.0\n",
            "Episode 83: Reward == 20.0\n",
            "Episode 84: Reward == 8.0\n",
            "Episode 85: Reward == 7.0\n",
            "Episode 86: Reward == 22.0\n",
            "Episode 87: Reward == 12.0\n",
            "Episode 88: Reward == 6.0\n",
            "Episode 89: Reward == 16.0\n",
            "Episode 90: Reward == 14.0\n",
            "Episode 91: Reward == 10.0\n",
            "Episode 92: Reward == 6.0\n",
            "Episode 93: Reward == 7.0\n",
            "Episode 94: Reward == 12.0\n",
            "Episode 95: Reward == 14.0\n",
            "Episode 96: Reward == 8.0\n",
            "Episode 97: Reward == 12.0\n",
            "Episode 98: Reward == 12.0\n",
            "Episode 99: Reward == 10.0\n",
            "Episode 100: Reward == 5.0\n",
            "Average reward: 10.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the baseline is around 150 unclipped / 9 clipped."
      ],
      "metadata": {
        "id": "q7VwVGfhc-p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "KTzMWfxFdDrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Convolution2D, Resizing, Rescaling, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.image import rgb_to_grayscale\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.utils import register_keras_serializable"
      ],
      "metadata": {
        "id": "MFF_4nRjdJOy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable(\"atari\")\n",
        "class GrayscaleLayer(Layer):\n",
        "  def call(self, input):\n",
        "    return rgb_to_grayscale(input)\n"
      ],
      "metadata": {
        "id": "YWjHyJ6T3Bof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(window_size, height, width, channels, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(window_size, height, width, channels)))\n",
        "    model.add(Reshape((window_size * height, width, channels), name=\"reshape_stack\"))\n",
        "    model.add(GrayscaleLayer(name=\"grayscale\"))\n",
        "    model.add(Resizing((window_size * height) // 2, width // 2, name=\"resize_half\"))\n",
        "    model.add(Rescaling(1./255, name=\"normalize\")) # normalize to [0, 1]\n",
        "    model.add(Reshape((window_size, height // 2, width // 2, 1), name=\"reshape_unstack\"))\n",
        "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', name=\"conv1\"))\n",
        "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu', name=\"conv2\"))\n",
        "    model.add(Convolution2D(64, (3,3), activation='relu', name=\"conv3\"))\n",
        "    model.add(Flatten(name=\"flatten\"))\n",
        "    model.add(Dense(512, activation='relu', name=\"fully_connected_1\"))\n",
        "    # model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear', name=\"output\"))\n",
        "    return model"
      ],
      "metadata": {
        "id": "_QXMNUladM_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 4\n",
        "height, width, channels = env.observation_space.shape\n",
        "actions = env.action_space.n"
      ],
      "metadata": {
        "id": "6rdDUGF1dgVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(WINDOW_SIZE, height, width, channels, actions)"
      ],
      "metadata": {
        "id": "t2YjqWDYdSp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ_R6xoWdolT",
        "outputId": "4e6e1311-e274-4839-d9da-2327ddc268ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"seqmodel\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 840, 160, 3)       0         \n",
            "                                                                 \n",
            " grayscale (GrayscaleLayer)  (None, 840, 160, 1)       0         \n",
            "                                                                 \n",
            " resizing (Resizing)         (None, 420, 80, 1)        0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 420, 80, 1)        0         \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 4, 105, 80, 1)     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 4, 25, 19, 32)     2080      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 4, 11, 8, 64)      32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 9, 6, 64)       36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 13824)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               7078400   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,153,318\n",
            "Trainable params: 7,153,318\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "m9i95BBkdzhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.processors import Processor\n",
        "from numpy import clip"
      ],
      "metadata": {
        "id": "FjrTen__d3Ip"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AtariRewardProcessor(Processor):\n",
        "  def process_reward(self, reward):\n",
        "      return clip(reward, -1.0, 1.0)"
      ],
      "metadata": {
        "id": "RgjgSj_nDnBw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions, window_size):\n",
        "    policy = LinearAnnealedPolicy(\n",
        "        EpsGreedyQPolicy(), \n",
        "        attr='eps', \n",
        "        value_max=1.0, \n",
        "        value_min=0.1, \n",
        "        value_test=0.2, \n",
        "        nb_steps=75000\n",
        "    )\n",
        "    memory = SequentialMemory(\n",
        "        limit=1000000, \n",
        "        window_length=window_size\n",
        "    )\n",
        "    dqn = DQNAgent(\n",
        "        model=model, \n",
        "        memory=memory, \n",
        "        policy=policy,\n",
        "        processor=AtariRewardProcessor(),\n",
        "        enable_dueling_network=True, \n",
        "        dueling_type='avg', \n",
        "        nb_actions=actions, \n",
        "        nb_steps_warmup=10000,\n",
        "        gamma=0.99\n",
        "    )\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "un0NM5Cee05L"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, actions, WINDOW_SIZE)\n",
        "dqn.compile(Adam(learning_rate=0.00025))"
      ],
      "metadata": {
        "id": "XCK4WyX1f1VF",
        "outputId": "8b1a224e-27bd-4813-c099-fd4063d3cecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c8d3d4e04104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWINDOW_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "rTFv2KMAimBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOP8OrohiUj6",
        "outputId": "12d7ef3f-9081-4010-f3c7-2fe14f902a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  545/10000: episode: 1, duration: 16.783s, episode steps: 545, steps per second:  32, episode reward: 120.000, mean reward:  0.220 [ 0.000, 30.000], mean action: 2.461 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1906/10000: episode: 2, duration: 1463.794s, episode steps: 1361, steps per second:   1, episode reward: 605.000, mean reward:  0.445 [ 0.000, 200.000], mean action: 2.497 [0.000, 5.000],  loss: 8.339929, mean_q: 0.921901, mean_eps: 0.869230\n",
            " 2575/10000: episode: 3, duration: 1071.301s, episode steps: 669, steps per second:   1, episode reward: 110.000, mean reward:  0.164 [ 0.000, 25.000], mean action: 2.469 [0.000, 5.000],  loss: 14.594663, mean_q: 1.724796, mean_eps: 0.798400\n",
            " 4166/10000: episode: 4, duration: 2526.268s, episode steps: 1591, steps per second:   1, episode reward: 250.000, mean reward:  0.157 [ 0.000, 30.000], mean action: 2.456 [0.000, 5.000],  loss: 2.485551, mean_q: 0.938030, mean_eps: 0.696700\n",
            " 4569/10000: episode: 5, duration: 642.676s, episode steps: 403, steps per second:   1, episode reward: 55.000, mean reward:  0.136 [ 0.000, 20.000], mean action: 2.546 [0.000, 5.000],  loss: 0.923119, mean_q: 0.454591, mean_eps: 0.606970\n",
            " 5166/10000: episode: 6, duration: 951.596s, episode steps: 597, steps per second:   1, episode reward: 175.000, mean reward:  0.293 [ 0.000, 30.000], mean action: 2.467 [0.000, 5.000],  loss: 1.286257, mean_q: 0.594287, mean_eps: 0.561970\n",
            " 5668/10000: episode: 7, duration: 802.174s, episode steps: 502, steps per second:   1, episode reward: 130.000, mean reward:  0.259 [ 0.000, 30.000], mean action: 2.239 [0.000, 5.000],  loss: 1.842880, mean_q: 1.066780, mean_eps: 0.512515\n",
            " 6085/10000: episode: 8, duration: 664.656s, episode steps: 417, steps per second:   1, episode reward: 55.000, mean reward:  0.132 [ 0.000, 20.000], mean action: 2.252 [0.000, 5.000],  loss: 1.886560, mean_q: 1.025469, mean_eps: 0.471160\n",
            " 6693/10000: episode: 9, duration: 966.496s, episode steps: 608, steps per second:   1, episode reward: 145.000, mean reward:  0.238 [ 0.000, 30.000], mean action: 2.521 [0.000, 5.000],  loss: 1.815734, mean_q: 0.812441, mean_eps: 0.425035\n",
            " 7356/10000: episode: 10, duration: 1052.600s, episode steps: 663, steps per second:   1, episode reward: 75.000, mean reward:  0.113 [ 0.000, 25.000], mean action: 2.867 [0.000, 5.000],  loss: 1.978757, mean_q: 0.818287, mean_eps: 0.367840\n",
            " 8047/10000: episode: 11, duration: 1095.542s, episode steps: 691, steps per second:   1, episode reward: 170.000, mean reward:  0.246 [ 0.000, 25.000], mean action: 2.700 [0.000, 5.000],  loss: 1.064034, mean_q: 0.558667, mean_eps: 0.306910\n",
            " 9161/10000: episode: 12, duration: 1751.153s, episode steps: 1114, steps per second:   1, episode reward: 270.000, mean reward:  0.242 [ 0.000, 30.000], mean action: 2.581 [0.000, 5.000],  loss: 1.322051, mean_q: 0.697359, mean_eps: 0.225685\n",
            " 9677/10000: episode: 13, duration: 813.462s, episode steps: 516, steps per second:   1, episode reward: 85.000, mean reward:  0.165 [ 0.000, 20.000], mean action: 2.382 [0.000, 5.000],  loss: 1.574947, mean_q: 0.554374, mean_eps: 0.152335\n",
            "done, took 14326.813 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd0d80f7f50>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn.test(env, nb_episodes=20, visualize=False)\n",
        "np.mean(scores.history[\"episode_reward\"])"
      ],
      "metadata": {
        "id": "pxnlZ2DdqMmO",
        "outputId": "65be622b-8feb-4b11-c899-fc4d6bef1281",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 40.000, steps: 694\n",
            "Episode 2: reward: 230.000, steps: 826\n",
            "Episode 3: reward: 25.000, steps: 570\n",
            "Episode 4: reward: 60.000, steps: 792\n",
            "Episode 5: reward: 35.000, steps: 557\n",
            "Episode 6: reward: 20.000, steps: 661\n",
            "Episode 7: reward: 225.000, steps: 967\n",
            "Episode 8: reward: 115.000, steps: 844\n",
            "Episode 9: reward: 80.000, steps: 677\n",
            "Episode 10: reward: 85.000, steps: 545\n",
            "Episode 11: reward: 75.000, steps: 672\n",
            "Episode 12: reward: 90.000, steps: 1360\n",
            "Episode 13: reward: 50.000, steps: 375\n",
            "Episode 14: reward: 105.000, steps: 1188\n",
            "Episode 15: reward: 80.000, steps: 683\n",
            "Episode 16: reward: 15.000, steps: 497\n",
            "Episode 17: reward: 340.000, steps: 988\n",
            "Episode 18: reward: 10.000, steps: 734\n",
            "Episode 19: reward: 65.000, steps: 435\n",
            "Episode 20: reward: 20.000, steps: 407\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88.25"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: \n",
        "* Train with 1.5M~2M steps\n",
        "* Add fourth conv layer\n",
        "* Don't use grayscale layer but initialize the environment with the grayscale option instead"
      ],
      "metadata": {
        "id": "_Zj_q_agK099"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAM-based approach\n",
        "\n",
        "Atari 2600 uses a 128 byte RAM to for its internal representation of the game state."
      ],
      "metadata": {
        "id": "kQxfb-PjLDCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envram = gym.make(\"SpaceInvaders-v4\", obs_type=\"ram\")"
      ],
      "metadata": {
        "id": "gd155WB7LbZC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envram.observation_space.shape"
      ],
      "metadata": {
        "id": "C8yq6U0oPk8t",
        "outputId": "28fb0fa3-80c6-4c59-a0e3-9c0c20956a5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ram_model(window_size, ram_size, actions):\n",
        "    model = Sequential(name=\"ram_model\")\n",
        "    model.add(Input(shape=(window_size, ram_size)))\n",
        "    model.add(Flatten(name=\"flatten\"))\n",
        "    model.add(Dense(512, activation=\"relu\", name=\"fc1\"))\n",
        "    model.add(Dense(128, activation=\"relu\", name=\"fc2\"))\n",
        "    model.add(Dense(actions, activation=\"linear\", name=\"output\"))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "NE0cVmGINY84"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 3\n",
        "ram_model = build_ram_model(WINDOW_SIZE, envram.observation_space.shape[0], envram.action_space.n)"
      ],
      "metadata": {
        "id": "ukerN3RGPZED"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ram_model.summary()"
      ],
      "metadata": {
        "id": "6LfVsYVgPxsY",
        "outputId": "2009b79c-6c68-454e-ca43-208f93d83843",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ram_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 384)               0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 512)               197120    \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 128)               65664     \n",
            "                                                                 \n",
            " output (Dense)              (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 263,558\n",
            "Trainable params: 263,558\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ram_dqn = build_agent(ram_model, envram.action_space.n, WINDOW_SIZE)"
      ],
      "metadata": {
        "id": "Q5pHvJr4P-Fz"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ram_dqn.compile(Adam(learning_rate=0.0002))"
      ],
      "metadata": {
        "id": "z7Ysz95-QGpX"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = ram_dqn.fit(envram, nb_steps=100_000, visualize=False, verbose=2)"
      ],
      "metadata": {
        "id": "SA3HzQueQRIZ",
        "outputId": "e542b84e-48aa-4022-b13e-c8f99a93b827",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   995/100000: episode: 1, duration: 2.017s, episode steps: 995, steps per second: 493, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  1744/100000: episode: 2, duration: 1.395s, episode steps: 749, steps per second: 537, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  2420/100000: episode: 3, duration: 1.263s, episode steps: 676, steps per second: 535, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  3217/100000: episode: 4, duration: 1.467s, episode steps: 797, steps per second: 543, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  3693/100000: episode: 5, duration: 0.866s, episode steps: 476, steps per second: 550, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.235 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  4593/100000: episode: 6, duration: 1.584s, episode steps: 900, steps per second: 568, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  5008/100000: episode: 7, duration: 0.726s, episode steps: 415, steps per second: 572, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  6181/100000: episode: 8, duration: 2.031s, episode steps: 1173, steps per second: 578, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  6804/100000: episode: 9, duration: 1.142s, episode steps: 623, steps per second: 546, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  7410/100000: episode: 10, duration: 1.078s, episode steps: 606, steps per second: 562, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  8244/100000: episode: 11, duration: 1.443s, episode steps: 834, steps per second: 578, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  9398/100000: episode: 12, duration: 2.082s, episode steps: 1154, steps per second: 554, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 10122/100000: episode: 13, duration: 3.754s, episode steps: 724, steps per second: 193, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 195.357807, mean_q: 75.091032, mean_eps: 0.879268\n",
            " 10854/100000: episode: 14, duration: 10.153s, episode steps: 732, steps per second:  72, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.551 [0.000, 5.000],  loss: 65.992800, mean_q: 65.533273, mean_eps: 0.874150\n",
            " 11619/100000: episode: 15, duration: 10.630s, episode steps: 765, steps per second:  72, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 51.122107, mean_q: 63.545574, mean_eps: 0.865168\n",
            " 12231/100000: episode: 16, duration: 8.476s, episode steps: 612, steps per second:  72, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 44.517604, mean_q: 62.400450, mean_eps: 0.856906\n",
            " 13168/100000: episode: 17, duration: 12.804s, episode steps: 937, steps per second:  73, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 41.119809, mean_q: 61.866202, mean_eps: 0.847612\n",
            " 13811/100000: episode: 18, duration: 8.823s, episode steps: 643, steps per second:  73, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 38.213634, mean_q: 61.302086, mean_eps: 0.838132\n",
            " 14520/100000: episode: 19, duration: 9.904s, episode steps: 709, steps per second:  72, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 37.806855, mean_q: 61.930175, mean_eps: 0.830020\n",
            " 15160/100000: episode: 20, duration: 8.808s, episode steps: 640, steps per second:  73, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 36.594269, mean_q: 61.487147, mean_eps: 0.821926\n",
            " 15692/100000: episode: 21, duration: 7.330s, episode steps: 532, steps per second:  73, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 34.740457, mean_q: 61.461203, mean_eps: 0.814894\n",
            " 16442/100000: episode: 22, duration: 10.264s, episode steps: 750, steps per second:  73, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 34.717786, mean_q: 61.346806, mean_eps: 0.807202\n",
            " 17218/100000: episode: 23, duration: 10.531s, episode steps: 776, steps per second:  74, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 33.805516, mean_q: 61.165924, mean_eps: 0.798046\n",
            " 17823/100000: episode: 24, duration: 8.364s, episode steps: 605, steps per second:  72, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 33.019631, mean_q: 61.560112, mean_eps: 0.789760\n",
            " 18741/100000: episode: 25, duration: 12.891s, episode steps: 918, steps per second:  71, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 31.892451, mean_q: 61.290279, mean_eps: 0.780622\n",
            " 19156/100000: episode: 26, duration: 5.776s, episode steps: 415, steps per second:  72, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 32.173700, mean_q: 61.602240, mean_eps: 0.772624\n",
            " 19797/100000: episode: 27, duration: 8.743s, episode steps: 641, steps per second:  73, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 32.013602, mean_q: 61.866722, mean_eps: 0.766288\n",
            " 20321/100000: episode: 28, duration: 7.312s, episode steps: 524, steps per second:  72, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 38.300404, mean_q: 63.754194, mean_eps: 0.759298\n",
            " 20950/100000: episode: 29, duration: 8.801s, episode steps: 629, steps per second:  71, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 35.619869, mean_q: 64.594007, mean_eps: 0.752380\n",
            " 21589/100000: episode: 30, duration: 8.633s, episode steps: 639, steps per second:  74, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 33.595648, mean_q: 63.966253, mean_eps: 0.744772\n",
            " 22524/100000: episode: 31, duration: 12.929s, episode steps: 935, steps per second:  72, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 31.759487, mean_q: 63.728155, mean_eps: 0.735328\n",
            " 23051/100000: episode: 32, duration: 7.415s, episode steps: 527, steps per second:  71, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.670 [0.000, 5.000],  loss: 31.943873, mean_q: 63.706968, mean_eps: 0.726556\n",
            " 23818/100000: episode: 33, duration: 10.613s, episode steps: 767, steps per second:  72, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 31.840009, mean_q: 63.855783, mean_eps: 0.718792\n",
            " 24646/100000: episode: 34, duration: 11.683s, episode steps: 828, steps per second:  71, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 31.207408, mean_q: 64.732190, mean_eps: 0.709222\n",
            " 25282/100000: episode: 35, duration: 8.775s, episode steps: 636, steps per second:  72, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 29.943701, mean_q: 64.463687, mean_eps: 0.700438\n",
            " 25856/100000: episode: 36, duration: 8.040s, episode steps: 574, steps per second:  71, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: 30.241570, mean_q: 64.075513, mean_eps: 0.693178\n",
            " 26388/100000: episode: 37, duration: 7.646s, episode steps: 532, steps per second:  70, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 30.382383, mean_q: 64.147790, mean_eps: 0.686542\n",
            " 27223/100000: episode: 38, duration: 12.217s, episode steps: 835, steps per second:  68, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 30.079123, mean_q: 64.047294, mean_eps: 0.678340\n",
            " 27765/100000: episode: 39, duration: 7.507s, episode steps: 542, steps per second:  72, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 30.446076, mean_q: 63.838803, mean_eps: 0.670078\n",
            " 28599/100000: episode: 40, duration: 11.524s, episode steps: 834, steps per second:  72, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 30.038816, mean_q: 64.243386, mean_eps: 0.661822\n",
            " 29270/100000: episode: 41, duration: 9.536s, episode steps: 671, steps per second:  70, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 29.624182, mean_q: 63.791899, mean_eps: 0.652792\n",
            " 30006/100000: episode: 42, duration: 10.487s, episode steps: 736, steps per second:  70, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 29.217367, mean_q: 63.890054, mean_eps: 0.644350\n",
            " 30534/100000: episode: 43, duration: 7.399s, episode steps: 528, steps per second:  71, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 17.359472, mean_q: 65.256818, mean_eps: 0.636766\n",
            " 31197/100000: episode: 44, duration: 9.622s, episode steps: 663, steps per second:  69, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 16.820764, mean_q: 65.275933, mean_eps: 0.629620\n",
            " 31865/100000: episode: 45, duration: 9.648s, episode steps: 668, steps per second:  69, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 16.557894, mean_q: 65.579569, mean_eps: 0.621634\n",
            " 32789/100000: episode: 46, duration: 13.141s, episode steps: 924, steps per second:  70, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 15.956232, mean_q: 65.346689, mean_eps: 0.612082\n",
            " 33181/100000: episode: 47, duration: 5.540s, episode steps: 392, steps per second:  71, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 16.301844, mean_q: 65.634468, mean_eps: 0.604186\n",
            " 33569/100000: episode: 48, duration: 5.392s, episode steps: 388, steps per second:  72, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 16.071560, mean_q: 65.709727, mean_eps: 0.599506\n",
            " 34169/100000: episode: 49, duration: 8.328s, episode steps: 600, steps per second:  72, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 15.373331, mean_q: 65.423070, mean_eps: 0.593578\n",
            " 34747/100000: episode: 50, duration: 8.017s, episode steps: 578, steps per second:  72, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 14.992350, mean_q: 65.605901, mean_eps: 0.586510\n",
            " 35168/100000: episode: 51, duration: 6.005s, episode steps: 421, steps per second:  70, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 15.648362, mean_q: 65.738583, mean_eps: 0.580516\n",
            " 35580/100000: episode: 52, duration: 5.855s, episode steps: 412, steps per second:  70, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 14.728163, mean_q: 65.810274, mean_eps: 0.575518\n",
            " 35976/100000: episode: 53, duration: 5.660s, episode steps: 396, steps per second:  70, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 14.686547, mean_q: 65.813712, mean_eps: 0.570670\n",
            " 36385/100000: episode: 54, duration: 5.680s, episode steps: 409, steps per second:  72, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 15.638628, mean_q: 65.924963, mean_eps: 0.565840\n",
            " 36987/100000: episode: 55, duration: 8.656s, episode steps: 602, steps per second:  70, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 15.554217, mean_q: 65.770806, mean_eps: 0.559774\n",
            " 37569/100000: episode: 56, duration: 8.278s, episode steps: 582, steps per second:  70, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 15.610501, mean_q: 66.158843, mean_eps: 0.552670\n",
            " 38137/100000: episode: 57, duration: 7.968s, episode steps: 568, steps per second:  71, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 15.628651, mean_q: 66.100673, mean_eps: 0.545770\n",
            " 38741/100000: episode: 58, duration: 8.477s, episode steps: 604, steps per second:  71, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 15.649566, mean_q: 66.421501, mean_eps: 0.538738\n",
            " 39205/100000: episode: 59, duration: 6.616s, episode steps: 464, steps per second:  70, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 15.851797, mean_q: 66.586581, mean_eps: 0.532330\n",
            " 40241/100000: episode: 60, duration: 15.033s, episode steps: 1036, steps per second:  69, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 14.278949, mean_q: 66.779197, mean_eps: 0.523330\n",
            " 40700/100000: episode: 61, duration: 6.602s, episode steps: 459, steps per second:  70, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 8.369260, mean_q: 67.111118, mean_eps: 0.514360\n",
            " 41068/100000: episode: 62, duration: 5.345s, episode steps: 368, steps per second:  69, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 7.883931, mean_q: 66.798837, mean_eps: 0.509398\n",
            " 41884/100000: episode: 63, duration: 11.752s, episode steps: 816, steps per second:  69, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 8.344223, mean_q: 67.029080, mean_eps: 0.502294\n",
            " 42963/100000: episode: 64, duration: 15.304s, episode steps: 1079, steps per second:  71, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 7.820164, mean_q: 67.307804, mean_eps: 0.490924\n",
            " 43786/100000: episode: 65, duration: 11.676s, episode steps: 823, steps per second:  70, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 7.799676, mean_q: 66.926728, mean_eps: 0.479512\n",
            " 44748/100000: episode: 66, duration: 13.681s, episode steps: 962, steps per second:  70, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 7.721763, mean_q: 66.738748, mean_eps: 0.468802\n",
            " 45699/100000: episode: 67, duration: 13.573s, episode steps: 951, steps per second:  70, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 7.505322, mean_q: 67.125264, mean_eps: 0.457324\n",
            " 46123/100000: episode: 68, duration: 6.034s, episode steps: 424, steps per second:  70, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 7.051205, mean_q: 67.009770, mean_eps: 0.449074\n",
            " 46833/100000: episode: 69, duration: 10.120s, episode steps: 710, steps per second:  70, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 7.599374, mean_q: 66.924489, mean_eps: 0.442270\n",
            " 47636/100000: episode: 70, duration: 11.566s, episode steps: 803, steps per second:  69, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 7.564609, mean_q: 66.777800, mean_eps: 0.433192\n",
            " 48304/100000: episode: 71, duration: 9.569s, episode steps: 668, steps per second:  70, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 7.287241, mean_q: 66.849779, mean_eps: 0.424366\n",
            " 49006/100000: episode: 72, duration: 10.098s, episode steps: 702, steps per second:  70, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 7.535314, mean_q: 66.695108, mean_eps: 0.416146\n",
            " 49937/100000: episode: 73, duration: 13.826s, episode steps: 931, steps per second:  67, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 6.868212, mean_q: 66.831051, mean_eps: 0.406348\n",
            " 50395/100000: episode: 74, duration: 6.886s, episode steps: 458, steps per second:  67, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.157 [0.000, 5.000],  loss: 6.377629, mean_q: 66.386702, mean_eps: 0.398014\n",
            " 50757/100000: episode: 75, duration: 5.426s, episode steps: 362, steps per second:  67, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 5.870115, mean_q: 65.944216, mean_eps: 0.393094\n",
            " 51553/100000: episode: 76, duration: 11.869s, episode steps: 796, steps per second:  67, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 5.681881, mean_q: 65.962270, mean_eps: 0.386146\n",
            " 52076/100000: episode: 77, duration: 7.768s, episode steps: 523, steps per second:  67, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 6.038970, mean_q: 65.764189, mean_eps: 0.378232\n",
            " 53005/100000: episode: 78, duration: 13.394s, episode steps: 929, steps per second:  69, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.095 [0.000, 5.000],  loss: 5.990220, mean_q: 66.074823, mean_eps: 0.369520\n",
            " 53562/100000: episode: 79, duration: 8.625s, episode steps: 557, steps per second:  65, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 5.612492, mean_q: 65.860722, mean_eps: 0.360604\n",
            " 54437/100000: episode: 80, duration: 13.232s, episode steps: 875, steps per second:  66, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 4.859595, mean_q: 66.162299, mean_eps: 0.352012\n",
            " 54878/100000: episode: 81, duration: 6.451s, episode steps: 441, steps per second:  68, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 5.065237, mean_q: 66.185563, mean_eps: 0.344116\n",
            " 55386/100000: episode: 82, duration: 7.426s, episode steps: 508, steps per second:  68, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 5.922327, mean_q: 65.970347, mean_eps: 0.338422\n",
            " 56395/100000: episode: 83, duration: 14.647s, episode steps: 1009, steps per second:  69, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 5.575492, mean_q: 66.137332, mean_eps: 0.329320\n",
            " 56775/100000: episode: 84, duration: 5.564s, episode steps: 380, steps per second:  68, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 5.227966, mean_q: 65.868437, mean_eps: 0.320986\n",
            " 57251/100000: episode: 85, duration: 6.827s, episode steps: 476, steps per second:  70, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.113 [0.000, 5.000],  loss: 5.229492, mean_q: 66.339754, mean_eps: 0.315850\n",
            " 57909/100000: episode: 86, duration: 9.455s, episode steps: 658, steps per second:  70, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 4.883676, mean_q: 66.447417, mean_eps: 0.309046\n",
            " 58305/100000: episode: 87, duration: 5.688s, episode steps: 396, steps per second:  70, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 5.434470, mean_q: 66.427151, mean_eps: 0.302722\n",
            " 58937/100000: episode: 88, duration: 9.194s, episode steps: 632, steps per second:  69, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 5.439986, mean_q: 66.471394, mean_eps: 0.296554\n",
            " 59793/100000: episode: 89, duration: 12.473s, episode steps: 856, steps per second:  69, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 5.315404, mean_q: 66.635019, mean_eps: 0.287626\n",
            " 60192/100000: episode: 90, duration: 6.036s, episode steps: 399, steps per second:  66, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 4.856530, mean_q: 66.680218, mean_eps: 0.280096\n",
            " 60847/100000: episode: 91, duration: 9.704s, episode steps: 655, steps per second:  67, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 4.240883, mean_q: 66.425035, mean_eps: 0.273772\n",
            " 61363/100000: episode: 92, duration: 7.559s, episode steps: 516, steps per second:  68, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 5.477410, mean_q: 66.125067, mean_eps: 0.266746\n",
            " 61983/100000: episode: 93, duration: 9.092s, episode steps: 620, steps per second:  68, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 4.677957, mean_q: 66.114336, mean_eps: 0.259930\n",
            " 62377/100000: episode: 94, duration: 5.854s, episode steps: 394, steps per second:  67, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.137 [0.000, 5.000],  loss: 5.759055, mean_q: 66.463877, mean_eps: 0.253846\n",
            " 62927/100000: episode: 95, duration: 8.023s, episode steps: 550, steps per second:  69, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.096 [0.000, 5.000],  loss: 4.580062, mean_q: 66.192875, mean_eps: 0.248182\n",
            " 63338/100000: episode: 96, duration: 5.929s, episode steps: 411, steps per second:  69, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 4.586847, mean_q: 66.166912, mean_eps: 0.242416\n",
            " 64111/100000: episode: 97, duration: 11.428s, episode steps: 773, steps per second:  68, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.930 [0.000, 5.000],  loss: 4.875125, mean_q: 66.185560, mean_eps: 0.235312\n",
            " 64706/100000: episode: 98, duration: 8.735s, episode steps: 595, steps per second:  68, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 4.090193, mean_q: 66.203582, mean_eps: 0.227104\n",
            " 65360/100000: episode: 99, duration: 9.615s, episode steps: 654, steps per second:  68, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.995 [0.000, 5.000],  loss: 3.833734, mean_q: 66.219020, mean_eps: 0.219610\n",
            " 66107/100000: episode: 100, duration: 11.028s, episode steps: 747, steps per second:  68, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 4.081897, mean_q: 65.961677, mean_eps: 0.211204\n",
            " 66954/100000: episode: 101, duration: 12.695s, episode steps: 847, steps per second:  67, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.826 [0.000, 5.000],  loss: 4.411649, mean_q: 65.990332, mean_eps: 0.201640\n",
            " 67800/100000: episode: 102, duration: 12.429s, episode steps: 846, steps per second:  68, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 3.662205, mean_q: 66.111060, mean_eps: 0.191482\n",
            " 68741/100000: episode: 103, duration: 13.973s, episode steps: 941, steps per second:  67, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.939 [0.000, 5.000],  loss: 4.558408, mean_q: 65.989319, mean_eps: 0.180760\n",
            " 69258/100000: episode: 104, duration: 7.474s, episode steps: 517, steps per second:  69, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 4.684080, mean_q: 66.052169, mean_eps: 0.172012\n",
            " 69910/100000: episode: 105, duration: 9.508s, episode steps: 652, steps per second:  69, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.615 [0.000, 5.000],  loss: 4.576251, mean_q: 66.090059, mean_eps: 0.164998\n",
            " 70527/100000: episode: 106, duration: 8.945s, episode steps: 617, steps per second:  69, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 4.386723, mean_q: 65.845696, mean_eps: 0.157384\n",
            " 71154/100000: episode: 107, duration: 9.012s, episode steps: 627, steps per second:  70, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 3.995033, mean_q: 65.699174, mean_eps: 0.149920\n",
            " 71800/100000: episode: 108, duration: 9.500s, episode steps: 646, steps per second:  68, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 3.483414, mean_q: 65.846608, mean_eps: 0.142282\n",
            " 72488/100000: episode: 109, duration: 10.326s, episode steps: 688, steps per second:  67, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 4.145671, mean_q: 65.745549, mean_eps: 0.134278\n",
            " 72834/100000: episode: 110, duration: 5.042s, episode steps: 346, steps per second:  69, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 3.316947, mean_q: 65.781867, mean_eps: 0.128074\n",
            " 73371/100000: episode: 111, duration: 7.915s, episode steps: 537, steps per second:  68, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 4.091456, mean_q: 65.938414, mean_eps: 0.122776\n",
            " 74299/100000: episode: 112, duration: 13.653s, episode steps: 928, steps per second:  68, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 3.972028, mean_q: 65.983691, mean_eps: 0.113986\n",
            " 74826/100000: episode: 113, duration: 7.832s, episode steps: 527, steps per second:  67, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 3.986586, mean_q: 66.024210, mean_eps: 0.105256\n",
            " 76036/100000: episode: 114, duration: 18.112s, episode steps: 1210, steps per second:  67, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.036 [0.000, 5.000],  loss: 3.734503, mean_q: 65.900926, mean_eps: 0.100151\n",
            " 76715/100000: episode: 115, duration: 10.246s, episode steps: 679, steps per second:  66, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 3.851562, mean_q: 65.978080, mean_eps: 0.100000\n",
            " 77231/100000: episode: 116, duration: 7.744s, episode steps: 516, steps per second:  67, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.868 [0.000, 5.000],  loss: 3.622267, mean_q: 65.919010, mean_eps: 0.100000\n",
            " 77921/100000: episode: 117, duration: 10.242s, episode steps: 690, steps per second:  67, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 3.955491, mean_q: 66.023641, mean_eps: 0.100000\n",
            " 78669/100000: episode: 118, duration: 11.131s, episode steps: 748, steps per second:  67, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.757 [0.000, 5.000],  loss: 3.248253, mean_q: 66.095838, mean_eps: 0.100000\n",
            " 79458/100000: episode: 119, duration: 11.802s, episode steps: 789, steps per second:  67, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 4.226204, mean_q: 65.835225, mean_eps: 0.100000\n",
            " 80814/100000: episode: 120, duration: 20.456s, episode steps: 1356, steps per second:  66, episode reward: 14.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.003 [0.000, 5.000],  loss: 3.386693, mean_q: 65.633516, mean_eps: 0.100000\n",
            " 81495/100000: episode: 121, duration: 10.026s, episode steps: 681, steps per second:  68, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 3.565574, mean_q: 65.396464, mean_eps: 0.100000\n",
            " 82592/100000: episode: 122, duration: 16.518s, episode steps: 1097, steps per second:  66, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 3.518620, mean_q: 65.348060, mean_eps: 0.100000\n",
            " 83603/100000: episode: 123, duration: 15.148s, episode steps: 1011, steps per second:  67, episode reward: 13.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 3.428568, mean_q: 65.388044, mean_eps: 0.100000\n",
            " 84693/100000: episode: 124, duration: 16.833s, episode steps: 1090, steps per second:  65, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 3.475867, mean_q: 65.166497, mean_eps: 0.100000\n",
            " 85310/100000: episode: 125, duration: 9.429s, episode steps: 617, steps per second:  65, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 3.265480, mean_q: 65.189237, mean_eps: 0.100000\n",
            " 85692/100000: episode: 126, duration: 5.911s, episode steps: 382, steps per second:  65, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 3.554036, mean_q: 65.036727, mean_eps: 0.100000\n",
            " 86312/100000: episode: 127, duration: 9.598s, episode steps: 620, steps per second:  65, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 2.731528, mean_q: 65.224673, mean_eps: 0.100000\n",
            " 87007/100000: episode: 128, duration: 10.885s, episode steps: 695, steps per second:  64, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.184 [0.000, 5.000],  loss: 3.070722, mean_q: 65.244148, mean_eps: 0.100000\n",
            " 88036/100000: episode: 129, duration: 15.967s, episode steps: 1029, steps per second:  64, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 3.261380, mean_q: 65.131959, mean_eps: 0.100000\n",
            " 88742/100000: episode: 130, duration: 10.777s, episode steps: 706, steps per second:  66, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.197 [0.000, 5.000],  loss: 3.133538, mean_q: 65.162492, mean_eps: 0.100000\n",
            " 89482/100000: episode: 131, duration: 11.239s, episode steps: 740, steps per second:  66, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.158 [0.000, 5.000],  loss: 2.959862, mean_q: 65.025723, mean_eps: 0.100000\n",
            " 90121/100000: episode: 132, duration: 9.758s, episode steps: 639, steps per second:  65, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.962 [0.000, 5.000],  loss: 3.552598, mean_q: 65.144823, mean_eps: 0.100000\n",
            " 90923/100000: episode: 133, duration: 12.247s, episode steps: 802, steps per second:  65, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 2.767019, mean_q: 65.065861, mean_eps: 0.100000\n",
            " 91898/100000: episode: 134, duration: 14.860s, episode steps: 975, steps per second:  66, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: 2.641964, mean_q: 65.119525, mean_eps: 0.100000\n",
            " 92607/100000: episode: 135, duration: 10.928s, episode steps: 709, steps per second:  65, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 2.813128, mean_q: 65.304961, mean_eps: 0.100000\n",
            " 93444/100000: episode: 136, duration: 12.707s, episode steps: 837, steps per second:  66, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 2.775206, mean_q: 65.532201, mean_eps: 0.100000\n",
            " 93834/100000: episode: 137, duration: 5.992s, episode steps: 390, steps per second:  65, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 3.678997, mean_q: 65.536003, mean_eps: 0.100000\n",
            " 94812/100000: episode: 138, duration: 14.831s, episode steps: 978, steps per second:  66, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 2.656439, mean_q: 65.337508, mean_eps: 0.100000\n",
            " 95526/100000: episode: 139, duration: 10.896s, episode steps: 714, steps per second:  66, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 3.115097, mean_q: 65.255332, mean_eps: 0.100000\n",
            " 96540/100000: episode: 140, duration: 16.115s, episode steps: 1014, steps per second:  63, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 2.874964, mean_q: 65.342989, mean_eps: 0.100000\n",
            " 97243/100000: episode: 141, duration: 10.713s, episode steps: 703, steps per second:  66, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 2.640576, mean_q: 65.393847, mean_eps: 0.100000\n",
            " 97955/100000: episode: 142, duration: 10.922s, episode steps: 712, steps per second:  65, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 2.812575, mean_q: 65.314489, mean_eps: 0.100000\n",
            " 98576/100000: episode: 143, duration: 9.429s, episode steps: 621, steps per second:  66, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 3.311338, mean_q: 65.564958, mean_eps: 0.100000\n",
            " 99372/100000: episode: 144, duration: 12.318s, episode steps: 796, steps per second:  65, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 2.374399, mean_q: 65.579409, mean_eps: 0.100000\n",
            " 99875/100000: episode: 145, duration: 8.000s, episode steps: 503, steps per second:  63, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 3.006278, mean_q: 65.615316, mean_eps: 0.100000\n",
            "done, took 1332.289 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = ram_dqn.test(envram, nb_episodes=100, visualize=False)\n",
        "np.mean(scores.history[\"episode_reward\"])"
      ],
      "metadata": {
        "id": "rTQ_pbQLRtc0",
        "outputId": "2ae99343-6bfc-449f-ad11-1e26710740e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: 10.000, steps: 811\n",
            "Episode 2: reward: 16.000, steps: 968\n",
            "Episode 3: reward: 20.000, steps: 1055\n",
            "Episode 4: reward: 12.000, steps: 848\n",
            "Episode 5: reward: 12.000, steps: 842\n",
            "Episode 6: reward: 18.000, steps: 1070\n",
            "Episode 7: reward: 16.000, steps: 967\n",
            "Episode 8: reward: 12.000, steps: 809\n",
            "Episode 9: reward: 12.000, steps: 840\n",
            "Episode 10: reward: 9.000, steps: 650\n",
            "Episode 11: reward: 12.000, steps: 852\n",
            "Episode 12: reward: 16.000, steps: 951\n",
            "Episode 13: reward: 10.000, steps: 785\n",
            "Episode 14: reward: 12.000, steps: 842\n",
            "Episode 15: reward: 12.000, steps: 835\n",
            "Episode 16: reward: 12.000, steps: 807\n",
            "Episode 17: reward: 17.000, steps: 955\n",
            "Episode 18: reward: 12.000, steps: 810\n",
            "Episode 19: reward: 11.000, steps: 813\n",
            "Episode 20: reward: 12.000, steps: 844\n",
            "Episode 21: reward: 11.000, steps: 807\n",
            "Episode 22: reward: 12.000, steps: 832\n",
            "Episode 23: reward: 12.000, steps: 858\n",
            "Episode 24: reward: 15.000, steps: 953\n",
            "Episode 25: reward: 12.000, steps: 808\n",
            "Episode 26: reward: 12.000, steps: 834\n",
            "Episode 27: reward: 12.000, steps: 847\n",
            "Episode 28: reward: 15.000, steps: 970\n",
            "Episode 29: reward: 25.000, steps: 1559\n",
            "Episode 30: reward: 11.000, steps: 817\n",
            "Episode 31: reward: 11.000, steps: 783\n",
            "Episode 32: reward: 10.000, steps: 807\n",
            "Episode 33: reward: 11.000, steps: 803\n",
            "Episode 34: reward: 11.000, steps: 842\n",
            "Episode 35: reward: 12.000, steps: 833\n",
            "Episode 36: reward: 11.000, steps: 801\n",
            "Episode 37: reward: 12.000, steps: 845\n",
            "Episode 38: reward: 13.000, steps: 819\n",
            "Episode 39: reward: 6.000, steps: 630\n",
            "Episode 40: reward: 15.000, steps: 968\n",
            "Episode 41: reward: 12.000, steps: 848\n",
            "Episode 42: reward: 12.000, steps: 798\n",
            "Episode 43: reward: 13.000, steps: 849\n",
            "Episode 44: reward: 16.000, steps: 947\n",
            "Episode 45: reward: 14.000, steps: 866\n",
            "Episode 46: reward: 11.000, steps: 845\n",
            "Episode 47: reward: 12.000, steps: 804\n",
            "Episode 48: reward: 12.000, steps: 840\n",
            "Episode 49: reward: 11.000, steps: 834\n",
            "Episode 50: reward: 12.000, steps: 822\n",
            "Episode 51: reward: 12.000, steps: 800\n",
            "Episode 52: reward: 12.000, steps: 834\n",
            "Episode 53: reward: 15.000, steps: 942\n",
            "Episode 54: reward: 15.000, steps: 982\n",
            "Episode 55: reward: 11.000, steps: 813\n",
            "Episode 56: reward: 6.000, steps: 634\n",
            "Episode 57: reward: 16.000, steps: 962\n",
            "Episode 58: reward: 12.000, steps: 797\n",
            "Episode 59: reward: 12.000, steps: 843\n",
            "Episode 60: reward: 11.000, steps: 836\n",
            "Episode 61: reward: 12.000, steps: 840\n",
            "Episode 62: reward: 12.000, steps: 841\n",
            "Episode 63: reward: 6.000, steps: 646\n",
            "Episode 64: reward: 12.000, steps: 800\n",
            "Episode 65: reward: 21.000, steps: 1407\n",
            "Episode 66: reward: 22.000, steps: 1434\n",
            "Episode 67: reward: 12.000, steps: 811\n",
            "Episode 68: reward: 18.000, steps: 1240\n",
            "Episode 69: reward: 10.000, steps: 771\n",
            "Episode 70: reward: 9.000, steps: 744\n",
            "Episode 71: reward: 12.000, steps: 808\n",
            "Episode 72: reward: 8.000, steps: 751\n",
            "Episode 73: reward: 9.000, steps: 763\n",
            "Episode 74: reward: 15.000, steps: 965\n",
            "Episode 75: reward: 12.000, steps: 797\n",
            "Episode 76: reward: 11.000, steps: 784\n",
            "Episode 77: reward: 12.000, steps: 822\n",
            "Episode 78: reward: 12.000, steps: 845\n",
            "Episode 79: reward: 11.000, steps: 830\n",
            "Episode 80: reward: 12.000, steps: 787\n",
            "Episode 81: reward: 16.000, steps: 953\n",
            "Episode 82: reward: 11.000, steps: 811\n",
            "Episode 83: reward: 11.000, steps: 793\n",
            "Episode 84: reward: 12.000, steps: 862\n",
            "Episode 85: reward: 12.000, steps: 813\n",
            "Episode 86: reward: 20.000, steps: 1062\n",
            "Episode 87: reward: 16.000, steps: 946\n",
            "Episode 88: reward: 9.000, steps: 790\n",
            "Episode 89: reward: 12.000, steps: 806\n",
            "Episode 90: reward: 16.000, steps: 974\n",
            "Episode 91: reward: 16.000, steps: 984\n",
            "Episode 92: reward: 12.000, steps: 806\n",
            "Episode 93: reward: 12.000, steps: 811\n",
            "Episode 94: reward: 16.000, steps: 951\n",
            "Episode 95: reward: 12.000, steps: 830\n",
            "Episode 96: reward: 12.000, steps: 804\n",
            "Episode 97: reward: 19.000, steps: 1018\n",
            "Episode 98: reward: 12.000, steps: 833\n",
            "Episode 99: reward: 15.000, steps: 947\n",
            "Episode 100: reward: 12.000, steps: 852\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.8"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ram_dqn.save_weights(\"ramdqn.h5f\", overwrite=True)"
      ],
      "metadata": {
        "id": "zWIrLGEz0axU"
      },
      "execution_count": 47,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "atari.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}