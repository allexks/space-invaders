{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2wsuq_uaCwE"
      },
      "source": [
        "# Space Invaders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies"
      ],
      "metadata": {
        "id": "8J9pbywRcrR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.8 gym keras-rl2 gym[atari]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hoe-hREJaEfO",
        "outputId": "693b9069-5e23-48f9-869c-81f72a1077e2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.8.0%2Bzzzcolab20220506162203-cp37-cp37m-linux_x86_64.whl\n",
            "\u001b[K     / 668.3 MB 113.2 MB/s\n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.12)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.26.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.21.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.0.0)\n",
            "Collecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 68.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (57.4.0)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.46.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (14.0.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym) (7.1.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (0.2.9)\n",
            "Installing collected packages: tf-estimator-nightly, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.1\n",
            "    Uninstalling tensorflow-2.9.1:\n",
            "      Successfully uninstalled tensorflow-2.9.1\n",
            "Successfully installed keras-2.8.0 tensorboard-2.8.0 tensorflow-2.8.0+zzzcolab20220506162203 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2  # fixes GPU issues"
      ],
      "metadata": {
        "id": "X1IwyVzBZFXe",
        "outputId": "1f8ccf51-9178-4f9f-d606-09adfb33f857",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8\n",
            "1 upgraded, 0 newly installed, 1 to remove and 43 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 3,139 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 9s (47.2 MB/s)\n",
            "(Reading database ... 155632 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\n",
            "(Reading database ... 155610 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROM instructions: https://github.com/openai/atari-py#roms"
      ],
      "metadata": {
        "id": "NPjXbngKZAX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m atari_py.import_roms roms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4iIN1BQZOGp",
        "outputId": "7a76e350-028e-40f2-b87a-95a4ec42c86c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "copying space_invaders.bin from roms/Space Invaders.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU?"
      ],
      "metadata": {
        "id": "ZBy-wBY8SHfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "tf.debugging.set_log_device_placement(True)"
      ],
      "metadata": {
        "id": "3vNNBk5WSI8H",
        "outputId": "90e93912-8129-41ca-b116-6f625af7e41c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploration and baseline"
      ],
      "metadata": {
        "id": "gS96fFQKcf86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7B2KcFLmcn6e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"SpaceInvaders-v4\")\n",
        "print(env.observation_space.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y074RXzxc1v7",
        "outputId": "479ef8e8-3278-4fd3-bc2b-eab6ee1e05e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(210, 160, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.unwrapped.get_action_meanings()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEd_Sh4nZfQU",
        "outputId": "445ad9cb-2e62-44b8-abbb-6c2e33b2bee5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import clip\n",
        "\n",
        "EPISODES = 100\n",
        "scores = []\n",
        "scores_clipped = []\n",
        "\n",
        "for episode in range(1, EPISODES + 1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    score_clipped = 0\n",
        "    \n",
        "    while not done:\n",
        "        # env.render()\n",
        "        action = random.choice(range(env.action_space.n))\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        score_clipped += clip(reward, -1.0, 1.0)\n",
        "    \n",
        "    scores.append(score)\n",
        "    scores_clipped.append(score_clipped)\n",
        "    print(f\"Episode {episode}: Reward == {score}; Clipped Reward == {score_clipped}\")\n",
        "\n",
        "avg = np.mean(scores)\n",
        "avg_clipped = np.mean(scores_clipped)\n",
        "print(f\"Average reward: {avg}; clipped: {avg_clipped}\")\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvgmfMu-Z3HG",
        "outputId": "9cb27106-cb21-4218-eca1-e48ead6fce44"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Reward == 120.0; Clipped Reward == 8.0\n",
            "Episode 2: Reward == 195.0; Clipped Reward == 13.0\n",
            "Episode 3: Reward == 180.0; Clipped Reward == 11.0\n",
            "Episode 4: Reward == 295.0; Clipped Reward == 17.0\n",
            "Episode 5: Reward == 120.0; Clipped Reward == 8.0\n",
            "Episode 6: Reward == 120.0; Clipped Reward == 8.0\n",
            "Episode 7: Reward == 365.0; Clipped Reward == 23.0\n",
            "Episode 8: Reward == 150.0; Clipped Reward == 10.0\n",
            "Episode 9: Reward == 485.0; Clipped Reward == 19.0\n",
            "Episode 10: Reward == 100.0; Clipped Reward == 8.0\n",
            "Episode 11: Reward == 260.0; Clipped Reward == 16.0\n",
            "Episode 12: Reward == 50.0; Clipped Reward == 4.0\n",
            "Episode 13: Reward == 135.0; Clipped Reward == 9.0\n",
            "Episode 14: Reward == 135.0; Clipped Reward == 9.0\n",
            "Episode 15: Reward == 110.0; Clipped Reward == 7.0\n",
            "Episode 16: Reward == 30.0; Clipped Reward == 3.0\n",
            "Episode 17: Reward == 105.0; Clipped Reward == 6.0\n",
            "Episode 18: Reward == 210.0; Clipped Reward == 12.0\n",
            "Episode 19: Reward == 185.0; Clipped Reward == 12.0\n",
            "Episode 20: Reward == 110.0; Clipped Reward == 7.0\n",
            "Episode 21: Reward == 140.0; Clipped Reward == 10.0\n",
            "Episode 22: Reward == 240.0; Clipped Reward == 15.0\n",
            "Episode 23: Reward == 135.0; Clipped Reward == 9.0\n",
            "Episode 24: Reward == 115.0; Clipped Reward == 8.0\n",
            "Episode 25: Reward == 125.0; Clipped Reward == 9.0\n",
            "Episode 26: Reward == 105.0; Clipped Reward == 8.0\n",
            "Episode 27: Reward == 230.0; Clipped Reward == 15.0\n",
            "Episode 28: Reward == 105.0; Clipped Reward == 6.0\n",
            "Episode 29: Reward == 140.0; Clipped Reward == 10.0\n",
            "Episode 30: Reward == 205.0; Clipped Reward == 16.0\n",
            "Episode 31: Reward == 130.0; Clipped Reward == 9.0\n",
            "Episode 32: Reward == 260.0; Clipped Reward == 16.0\n",
            "Episode 33: Reward == 105.0; Clipped Reward == 6.0\n",
            "Episode 34: Reward == 260.0; Clipped Reward == 16.0\n",
            "Episode 35: Reward == 215.0; Clipped Reward == 13.0\n",
            "Episode 36: Reward == 45.0; Clipped Reward == 3.0\n",
            "Episode 37: Reward == 180.0; Clipped Reward == 11.0\n",
            "Episode 38: Reward == 210.0; Clipped Reward == 12.0\n",
            "Episode 39: Reward == 110.0; Clipped Reward == 7.0\n",
            "Episode 40: Reward == 185.0; Clipped Reward == 12.0\n",
            "Episode 41: Reward == 140.0; Clipped Reward == 10.0\n",
            "Episode 42: Reward == 125.0; Clipped Reward == 10.0\n",
            "Episode 43: Reward == 180.0; Clipped Reward == 11.0\n",
            "Episode 44: Reward == 135.0; Clipped Reward == 9.0\n",
            "Episode 45: Reward == 745.0; Clipped Reward == 23.0\n",
            "Episode 46: Reward == 295.0; Clipped Reward == 20.0\n",
            "Episode 47: Reward == 110.0; Clipped Reward == 8.0\n",
            "Episode 48: Reward == 155.0; Clipped Reward == 10.0\n",
            "Episode 49: Reward == 75.0; Clipped Reward == 4.0\n",
            "Episode 50: Reward == 60.0; Clipped Reward == 5.0\n",
            "Episode 51: Reward == 215.0; Clipped Reward == 13.0\n",
            "Episode 52: Reward == 105.0; Clipped Reward == 6.0\n",
            "Episode 53: Reward == 135.0; Clipped Reward == 9.0\n",
            "Episode 54: Reward == 135.0; Clipped Reward == 9.0\n",
            "Episode 55: Reward == 215.0; Clipped Reward == 13.0\n",
            "Episode 56: Reward == 210.0; Clipped Reward == 12.0\n",
            "Episode 57: Reward == 360.0; Clipped Reward == 22.0\n",
            "Episode 58: Reward == 110.0; Clipped Reward == 7.0\n",
            "Episode 59: Reward == 20.0; Clipped Reward == 3.0\n",
            "Episode 60: Reward == 105.0; Clipped Reward == 8.0\n",
            "Episode 61: Reward == 120.0; Clipped Reward == 8.0\n",
            "Episode 62: Reward == 170.0; Clipped Reward == 12.0\n",
            "Episode 63: Reward == 155.0; Clipped Reward == 10.0\n",
            "Episode 64: Reward == 45.0; Clipped Reward == 5.0\n",
            "Episode 65: Reward == 135.0; Clipped Reward == 9.0\n",
            "Episode 66: Reward == 355.0; Clipped Reward == 11.0\n",
            "Episode 67: Reward == 190.0; Clipped Reward == 12.0\n",
            "Episode 68: Reward == 280.0; Clipped Reward == 18.0\n",
            "Episode 69: Reward == 110.0; Clipped Reward == 7.0\n",
            "Episode 70: Reward == 265.0; Clipped Reward == 17.0\n",
            "Episode 71: Reward == 155.0; Clipped Reward == 10.0\n",
            "Episode 72: Reward == 110.0; Clipped Reward == 7.0\n",
            "Episode 73: Reward == 120.0; Clipped Reward == 8.0\n",
            "Episode 74: Reward == 170.0; Clipped Reward == 13.0\n",
            "Episode 75: Reward == 240.0; Clipped Reward == 15.0\n",
            "Episode 76: Reward == 80.0; Clipped Reward == 6.0\n",
            "Episode 77: Reward == 180.0; Clipped Reward == 11.0\n",
            "Episode 78: Reward == 110.0; Clipped Reward == 7.0\n",
            "Episode 79: Reward == 75.0; Clipped Reward == 5.0\n",
            "Episode 80: Reward == 90.0; Clipped Reward == 9.0\n",
            "Episode 81: Reward == 180.0; Clipped Reward == 11.0\n",
            "Episode 82: Reward == 230.0; Clipped Reward == 15.0\n",
            "Episode 83: Reward == 275.0; Clipped Reward == 19.0\n",
            "Episode 84: Reward == 110.0; Clipped Reward == 7.0\n",
            "Episode 85: Reward == 110.0; Clipped Reward == 7.0\n",
            "Episode 86: Reward == 95.0; Clipped Reward == 6.0\n",
            "Episode 87: Reward == 105.0; Clipped Reward == 6.0\n",
            "Episode 88: Reward == 520.0; Clipped Reward == 24.0\n",
            "Episode 89: Reward == 15.0; Clipped Reward == 2.0\n",
            "Episode 90: Reward == 75.0; Clipped Reward == 6.0\n",
            "Episode 91: Reward == 65.0; Clipped Reward == 6.0\n",
            "Episode 92: Reward == 120.0; Clipped Reward == 8.0\n",
            "Episode 93: Reward == 75.0; Clipped Reward == 5.0\n",
            "Episode 94: Reward == 210.0; Clipped Reward == 12.0\n",
            "Episode 95: Reward == 45.0; Clipped Reward == 5.0\n",
            "Episode 96: Reward == 80.0; Clipped Reward == 6.0\n",
            "Episode 97: Reward == 570.0; Clipped Reward == 25.0\n",
            "Episode 98: Reward == 100.0; Clipped Reward == 8.0\n",
            "Episode 99: Reward == 120.0; Clipped Reward == 10.0\n",
            "Episode 100: Reward == 100.0; Clipped Reward == 8.0\n",
            "Average reward: 166.85; clipped: 10.29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the baseline is around 150 unclipped / 9-10 clipped.\n",
        "\n"
      ],
      "metadata": {
        "id": "q7VwVGfhc-p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "KTzMWfxFdDrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Convolution2D, Resizing, Rescaling, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.image import rgb_to_grayscale\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.utils import register_keras_serializable"
      ],
      "metadata": {
        "id": "MFF_4nRjdJOy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable(\"atari\")\n",
        "class GrayscaleLayer(Layer):\n",
        "  def call(self, input):\n",
        "    return rgb_to_grayscale(input)\n"
      ],
      "metadata": {
        "id": "YWjHyJ6T3Bof",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "a7d92ff9-b012-4a3c-8746-08dbe99d1353"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4f160e575552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mregister_keras_serializable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"atari\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGrayscaleLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdecorator\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregistered_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_GLOBAL_CUSTOM_OBJECTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m       raise ValueError(\n\u001b[0;32m--> 380\u001b[0;31m           \u001b[0;34mf'{registered_name} has already been registered to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m           f'{_GLOBAL_CUSTOM_OBJECTS[registered_name]}')\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: atari>GrayscaleLayer has already been registered to <class '__main__.GrayscaleLayer'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(window_size, height, width, channels, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(window_size, height, width, channels)))\n",
        "    model.add(Reshape((window_size * height, width, channels), name=\"reshape_stack\"))\n",
        "    model.add(GrayscaleLayer(name=\"grayscale\"))\n",
        "    model.add(Resizing((window_size * height) // 2, width // 2, name=\"resize_half\"))\n",
        "    model.add(Rescaling(1./255, name=\"normalize\")) # normalize to [0, 1]\n",
        "    model.add(Reshape((window_size, height // 2, width // 2, 1), name=\"reshape_unstack\"))\n",
        "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', name=\"conv1\"))\n",
        "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu', name=\"conv2\"))\n",
        "    model.add(Convolution2D(64, (3,3), activation='relu', name=\"conv3\"))\n",
        "    model.add(Flatten(name=\"flatten\"))\n",
        "    model.add(Dense(512, activation='relu', name=\"fully_connected_1\"))\n",
        "    # model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear', name=\"output\"))\n",
        "    return model"
      ],
      "metadata": {
        "id": "_QXMNUladM_T"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 4\n",
        "height, width, channels = env.observation_space.shape\n",
        "actions = env.action_space.n"
      ],
      "metadata": {
        "id": "6rdDUGF1dgVE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(WINDOW_SIZE, height, width, channels, actions)  "
      ],
      "metadata": {
        "id": "t2YjqWDYdSp5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ_R6xoWdolT",
        "outputId": "0e1b421e-b7a1-4e17-f8ae-cae878d0370a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape_stack (Reshape)     (None, 840, 160, 3)       0         \n",
            "                                                                 \n",
            " grayscale (GrayscaleLayer)  (None, 840, 160, 1)       0         \n",
            "                                                                 \n",
            " resize_half (Resizing)      (None, 420, 80, 1)        0         \n",
            "                                                                 \n",
            " normalize (Rescaling)       (None, 420, 80, 1)        0         \n",
            "                                                                 \n",
            " reshape_unstack (Reshape)   (None, 4, 105, 80, 1)     0         \n",
            "                                                                 \n",
            " conv1 (Conv2D)              (None, 4, 25, 19, 32)     2080      \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (None, 4, 11, 8, 64)      32832     \n",
            "                                                                 \n",
            " conv3 (Conv2D)              (None, 4, 9, 6, 64)       36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 13824)             0         \n",
            "                                                                 \n",
            " fully_connected_1 (Dense)   (None, 512)               7078400   \n",
            "                                                                 \n",
            " output (Dense)              (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,153,318\n",
            "Trainable params: 7,153,318\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent"
      ],
      "metadata": {
        "id": "m9i95BBkdzhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "from rl.processors import Processor\n",
        "from numpy import clip"
      ],
      "metadata": {
        "id": "FjrTen__d3Ip"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AtariRewardProcessor(Processor):\n",
        "  def process_reward(self, reward):\n",
        "      return clip(reward, -1.0, 1.0)"
      ],
      "metadata": {
        "id": "RgjgSj_nDnBw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_agent(model, actions, window_size):\n",
        "    policy = LinearAnnealedPolicy(\n",
        "        EpsGreedyQPolicy(), \n",
        "        attr='eps', \n",
        "        value_max=1.0, \n",
        "        value_min=0.1, \n",
        "        value_test=0.2, \n",
        "        nb_steps=75000\n",
        "    )\n",
        "    memory = SequentialMemory(\n",
        "        limit=1000000, \n",
        "        window_length=window_size\n",
        "    )\n",
        "    dqn = DQNAgent(\n",
        "        model=model, \n",
        "        memory=memory, \n",
        "        policy=policy,\n",
        "        processor=AtariRewardProcessor(),\n",
        "        enable_dueling_network=True, \n",
        "        dueling_type='avg', \n",
        "        nb_actions=actions, \n",
        "        nb_steps_warmup=10000,\n",
        "        gamma=0.99\n",
        "    )\n",
        "    return dqn"
      ],
      "metadata": {
        "id": "un0NM5Cee05L"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = build_agent(model, actions, WINDOW_SIZE)\n",
        "dqn.compile(Adam(learning_rate=0.00025))"
      ],
      "metadata": {
        "id": "XCK4WyX1f1VF"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "rTFv2KMAimBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOP8OrohiUj6",
        "outputId": "6aa0c6e9-8c5d-45f8-b569-a212dc38043b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  857/10000: episode: 1, duration: 16.724s, episode steps: 857, steps per second:  51, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done, took 76.677 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f04841117d0>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = dqn.test(env, nb_episodes=20, visualize=False)\n",
        "np.mean(scores.history[\"episode_reward\"])"
      ],
      "metadata": {
        "id": "pxnlZ2DdqMmO",
        "outputId": "65be622b-8feb-4b11-c899-fc4d6bef1281",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 40.000, steps: 694\n",
            "Episode 2: reward: 230.000, steps: 826\n",
            "Episode 3: reward: 25.000, steps: 570\n",
            "Episode 4: reward: 60.000, steps: 792\n",
            "Episode 5: reward: 35.000, steps: 557\n",
            "Episode 6: reward: 20.000, steps: 661\n",
            "Episode 7: reward: 225.000, steps: 967\n",
            "Episode 8: reward: 115.000, steps: 844\n",
            "Episode 9: reward: 80.000, steps: 677\n",
            "Episode 10: reward: 85.000, steps: 545\n",
            "Episode 11: reward: 75.000, steps: 672\n",
            "Episode 12: reward: 90.000, steps: 1360\n",
            "Episode 13: reward: 50.000, steps: 375\n",
            "Episode 14: reward: 105.000, steps: 1188\n",
            "Episode 15: reward: 80.000, steps: 683\n",
            "Episode 16: reward: 15.000, steps: 497\n",
            "Episode 17: reward: 340.000, steps: 988\n",
            "Episode 18: reward: 10.000, steps: 734\n",
            "Episode 19: reward: 65.000, steps: 435\n",
            "Episode 20: reward: 20.000, steps: 407\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88.25"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: \n",
        "* Train with 1.5M~2M steps\n",
        "* Add fourth conv layer\n",
        "* Don't use grayscale layer but initialize the environment with the grayscale option instead"
      ],
      "metadata": {
        "id": "_Zj_q_agK099"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAM-based approach\n",
        "\n",
        "Atari 2600 uses a 128 byte RAM to for its internal representation of the game state."
      ],
      "metadata": {
        "id": "kQxfb-PjLDCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "envram = gym.make(\"SpaceInvaders-v4\", obs_type=\"ram\")"
      ],
      "metadata": {
        "id": "gd155WB7LbZC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "envram.observation_space.shape"
      ],
      "metadata": {
        "id": "C8yq6U0oPk8t",
        "outputId": "6fb66d34-aa99-46b1-841f-45401f85eff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ram_model(window_size, ram_size, actions):\n",
        "    model = Sequential(name=\"ram_model\")\n",
        "    model.add(Input(shape=(window_size, ram_size)))\n",
        "    model.add(Flatten(name=\"flatten\"))\n",
        "    model.add(Dense(512, activation=\"relu\", name=\"fc1\"))\n",
        "    model.add(Dense(128, activation=\"relu\", name=\"fc2\"))\n",
        "    model.add(Dense(actions, activation=\"linear\", name=\"output\"))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "NE0cVmGINY84"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = 3\n",
        "ram_model = build_ram_model(WINDOW_SIZE, envram.observation_space.shape[0], envram.action_space.n)"
      ],
      "metadata": {
        "id": "ukerN3RGPZED"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ram_model.summary()"
      ],
      "metadata": {
        "id": "6LfVsYVgPxsY",
        "outputId": "34a6ad76-d21e-40e2-8b3f-1929af939145",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"ram_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 384)               0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 512)               197120    \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 128)               65664     \n",
            "                                                                 \n",
            " output (Dense)              (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 263,558\n",
            "Trainable params: 263,558\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ram_dqn = build_agent(ram_model, envram.action_space.n, WINDOW_SIZE)"
      ],
      "metadata": {
        "id": "Q5pHvJr4P-Fz"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ram_dqn.compile(Adam(learning_rate=0.0002))"
      ],
      "metadata": {
        "id": "z7Ysz95-QGpX"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = ram_dqn.fit(envram, nb_steps=100_000, visualize=False, verbose=2)"
      ],
      "metadata": {
        "id": "SA3HzQueQRIZ",
        "outputId": "c42bb3c4-47af-4572-c1b0-f653d21f7c63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   913/100000: episode: 1, duration: 2.158s, episode steps: 913, steps per second: 423, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  2291/100000: episode: 2, duration: 2.337s, episode steps: 1378, steps per second: 590, episode reward: 25.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  3081/100000: episode: 3, duration: 1.356s, episode steps: 790, steps per second: 583, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  4072/100000: episode: 4, duration: 1.690s, episode steps: 991, steps per second: 586, episode reward:  7.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  4844/100000: episode: 5, duration: 1.365s, episode steps: 772, steps per second: 565, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  5341/100000: episode: 6, duration: 0.955s, episode steps: 497, steps per second: 520, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  6169/100000: episode: 7, duration: 1.423s, episode steps: 828, steps per second: 582, episode reward:  7.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  7091/100000: episode: 8, duration: 1.587s, episode steps: 922, steps per second: 581, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  7482/100000: episode: 9, duration: 0.674s, episode steps: 391, steps per second: 580, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  8133/100000: episode: 10, duration: 1.134s, episode steps: 651, steps per second: 574, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  8558/100000: episode: 11, duration: 0.745s, episode steps: 425, steps per second: 571, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  8955/100000: episode: 12, duration: 0.723s, episode steps: 397, steps per second: 549, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
            "  9368/100000: episode: 13, duration: 0.766s, episode steps: 413, steps per second: 539, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 10012/100000: episode: 14, duration: 3.567s, episode steps: 644, steps per second: 181, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 6355.657504, mean_q: 244.676995, mean_eps: 0.879928\n",
            " 10657/100000: episode: 15, duration: 6.705s, episode steps: 645, steps per second:  96, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 295.729557, mean_q: 166.894596, mean_eps: 0.875992\n",
            " 11386/100000: episode: 16, duration: 7.619s, episode steps: 729, steps per second:  96, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 175.217151, mean_q: 162.289567, mean_eps: 0.867748\n",
            " 12235/100000: episode: 17, duration: 8.991s, episode steps: 849, steps per second:  94, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 174.615387, mean_q: 160.636613, mean_eps: 0.858280\n",
            " 12861/100000: episode: 18, duration: 6.613s, episode steps: 626, steps per second:  95, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 152.536890, mean_q: 160.702110, mean_eps: 0.849430\n",
            " 13455/100000: episode: 19, duration: 6.240s, episode steps: 594, steps per second:  95, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 152.487331, mean_q: 160.018902, mean_eps: 0.842110\n",
            " 14059/100000: episode: 20, duration: 6.296s, episode steps: 604, steps per second:  96, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 142.607593, mean_q: 159.593907, mean_eps: 0.834922\n",
            " 14500/100000: episode: 21, duration: 4.593s, episode steps: 441, steps per second:  96, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 141.074213, mean_q: 159.043853, mean_eps: 0.828652\n",
            " 14995/100000: episode: 22, duration: 5.307s, episode steps: 495, steps per second:  93, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 151.878202, mean_q: 159.136164, mean_eps: 0.823036\n",
            " 15434/100000: episode: 23, duration: 4.627s, episode steps: 439, steps per second:  95, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 136.881508, mean_q: 159.664596, mean_eps: 0.817432\n",
            " 16156/100000: episode: 24, duration: 7.540s, episode steps: 722, steps per second:  96, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 144.224810, mean_q: 159.429133, mean_eps: 0.810466\n",
            " 16753/100000: episode: 25, duration: 6.302s, episode steps: 597, steps per second:  95, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 144.471380, mean_q: 159.540579, mean_eps: 0.802552\n",
            " 17586/100000: episode: 26, duration: 8.848s, episode steps: 833, steps per second:  94, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 137.428604, mean_q: 158.834622, mean_eps: 0.793972\n",
            " 18363/100000: episode: 27, duration: 8.203s, episode steps: 777, steps per second:  95, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 139.874566, mean_q: 158.148640, mean_eps: 0.784312\n",
            " 19015/100000: episode: 28, duration: 6.845s, episode steps: 652, steps per second:  95, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 132.273077, mean_q: 158.370464, mean_eps: 0.775738\n",
            " 19688/100000: episode: 29, duration: 7.033s, episode steps: 673, steps per second:  96, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 131.250798, mean_q: 158.018131, mean_eps: 0.767788\n",
            " 20494/100000: episode: 30, duration: 8.618s, episode steps: 806, steps per second:  94, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 164.022400, mean_q: 160.270096, mean_eps: 0.758914\n",
            " 21739/100000: episode: 31, duration: 13.126s, episode steps: 1245, steps per second:  95, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 159.068539, mean_q: 161.133405, mean_eps: 0.746608\n",
            " 22178/100000: episode: 32, duration: 4.654s, episode steps: 439, steps per second:  94, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 140.807767, mean_q: 160.241996, mean_eps: 0.736504\n",
            " 23034/100000: episode: 33, duration: 9.039s, episode steps: 856, steps per second:  95, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 146.101595, mean_q: 159.913809, mean_eps: 0.728734\n",
            " 23643/100000: episode: 34, duration: 6.561s, episode steps: 609, steps per second:  93, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 134.455611, mean_q: 159.344742, mean_eps: 0.719944\n",
            " 24433/100000: episode: 35, duration: 9.095s, episode steps: 790, steps per second:  87, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 138.065022, mean_q: 158.887944, mean_eps: 0.711550\n",
            " 25140/100000: episode: 36, duration: 7.431s, episode steps: 707, steps per second:  95, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 141.054638, mean_q: 159.169724, mean_eps: 0.702568\n",
            " 26021/100000: episode: 37, duration: 9.427s, episode steps: 881, steps per second:  93, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 133.211101, mean_q: 158.645380, mean_eps: 0.693040\n",
            " 27164/100000: episode: 38, duration: 12.024s, episode steps: 1143, steps per second:  95, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 137.351034, mean_q: 158.065503, mean_eps: 0.680896\n",
            " 28254/100000: episode: 39, duration: 11.893s, episode steps: 1090, steps per second:  92, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 137.254033, mean_q: 157.483772, mean_eps: 0.667498\n",
            " 28851/100000: episode: 40, duration: 6.478s, episode steps: 597, steps per second:  92, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 127.417057, mean_q: 157.308430, mean_eps: 0.657376\n",
            " 29500/100000: episode: 41, duration: 6.909s, episode steps: 649, steps per second:  94, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 131.422808, mean_q: 156.963403, mean_eps: 0.649900\n",
            " 30380/100000: episode: 42, duration: 9.416s, episode steps: 880, steps per second:  93, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.270 [0.000, 5.000],  loss: 125.913415, mean_q: 159.550520, mean_eps: 0.640726\n",
            " 31046/100000: episode: 43, duration: 7.095s, episode steps: 666, steps per second:  94, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 106.303238, mean_q: 162.363947, mean_eps: 0.631450\n",
            " 31669/100000: episode: 44, duration: 6.741s, episode steps: 623, steps per second:  92, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 111.963077, mean_q: 162.420075, mean_eps: 0.623716\n",
            " 32424/100000: episode: 45, duration: 7.978s, episode steps: 755, steps per second:  95, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 106.467766, mean_q: 162.385849, mean_eps: 0.615448\n",
            " 32945/100000: episode: 46, duration: 5.577s, episode steps: 521, steps per second:  93, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 105.828795, mean_q: 162.058556, mean_eps: 0.607792\n",
            " 33658/100000: episode: 47, duration: 7.587s, episode steps: 713, steps per second:  94, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.202 [0.000, 5.000],  loss: 105.530524, mean_q: 162.597557, mean_eps: 0.600388\n",
            " 34935/100000: episode: 48, duration: 13.678s, episode steps: 1277, steps per second:  93, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 108.860334, mean_q: 162.610450, mean_eps: 0.588448\n",
            " 36251/100000: episode: 49, duration: 14.139s, episode steps: 1316, steps per second:  93, episode reward: 17.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 103.959983, mean_q: 162.744225, mean_eps: 0.572890\n",
            " 37151/100000: episode: 50, duration: 9.716s, episode steps: 900, steps per second:  93, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.119 [0.000, 5.000],  loss: 106.546076, mean_q: 161.744261, mean_eps: 0.559594\n",
            " 37820/100000: episode: 51, duration: 7.119s, episode steps: 669, steps per second:  94, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.006 [0.000, 5.000],  loss: 108.877863, mean_q: 161.745351, mean_eps: 0.550180\n",
            " 38527/100000: episode: 52, duration: 7.536s, episode steps: 707, steps per second:  94, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.031 [0.000, 5.000],  loss: 103.517658, mean_q: 161.346945, mean_eps: 0.541924\n",
            " 39334/100000: episode: 53, duration: 8.639s, episode steps: 807, steps per second:  93, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 100.058239, mean_q: 161.329411, mean_eps: 0.532840\n",
            " 40135/100000: episode: 54, duration: 8.690s, episode steps: 801, steps per second:  92, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.046 [0.000, 5.000],  loss: 90.841044, mean_q: 161.755742, mean_eps: 0.523192\n",
            " 40895/100000: episode: 55, duration: 8.143s, episode steps: 760, steps per second:  93, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.014 [0.000, 5.000],  loss: 47.160533, mean_q: 162.908882, mean_eps: 0.513826\n",
            " 41535/100000: episode: 56, duration: 6.906s, episode steps: 640, steps per second:  93, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 42.745848, mean_q: 162.349510, mean_eps: 0.505426\n",
            " 41929/100000: episode: 57, duration: 4.226s, episode steps: 394, steps per second:  93, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 49.002550, mean_q: 162.459365, mean_eps: 0.499222\n",
            " 42692/100000: episode: 58, duration: 8.932s, episode steps: 763, steps per second:  85, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 45.892737, mean_q: 162.866117, mean_eps: 0.492280\n",
            " 43147/100000: episode: 59, duration: 4.907s, episode steps: 455, steps per second:  93, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 48.532438, mean_q: 162.298817, mean_eps: 0.484972\n",
            " 43742/100000: episode: 60, duration: 6.401s, episode steps: 595, steps per second:  93, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 50.099173, mean_q: 162.393489, mean_eps: 0.478672\n",
            " 44509/100000: episode: 61, duration: 8.318s, episode steps: 767, steps per second:  92, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 43.561981, mean_q: 162.374197, mean_eps: 0.470500\n",
            " 45428/100000: episode: 62, duration: 9.885s, episode steps: 919, steps per second:  93, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.054 [0.000, 5.000],  loss: 43.608881, mean_q: 162.056761, mean_eps: 0.460384\n",
            " 45970/100000: episode: 63, duration: 5.808s, episode steps: 542, steps per second:  93, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 43.629630, mean_q: 162.123878, mean_eps: 0.451618\n",
            " 46362/100000: episode: 64, duration: 4.286s, episode steps: 392, steps per second:  91, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 51.335021, mean_q: 161.800440, mean_eps: 0.446014\n",
            " 47049/100000: episode: 65, duration: 7.385s, episode steps: 687, steps per second:  93, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.924 [0.000, 5.000],  loss: 46.476201, mean_q: 162.063322, mean_eps: 0.439540\n",
            " 47948/100000: episode: 66, duration: 9.766s, episode steps: 899, steps per second:  92, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 40.892745, mean_q: 162.244652, mean_eps: 0.430024\n",
            " 48335/100000: episode: 67, duration: 4.198s, episode steps: 387, steps per second:  92, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 41.390113, mean_q: 162.261670, mean_eps: 0.422308\n",
            " 49263/100000: episode: 68, duration: 9.940s, episode steps: 928, steps per second:  93, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 50.819839, mean_q: 162.286777, mean_eps: 0.414418\n",
            " 49628/100000: episode: 69, duration: 3.934s, episode steps: 365, steps per second:  93, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.178 [0.000, 5.000],  loss: 53.054243, mean_q: 162.033283, mean_eps: 0.406660\n",
            " 50240/100000: episode: 70, duration: 6.668s, episode steps: 612, steps per second:  92, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 44.495121, mean_q: 162.146979, mean_eps: 0.400798\n",
            " 51129/100000: episode: 71, duration: 9.740s, episode steps: 889, steps per second:  91, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 34.028057, mean_q: 162.238478, mean_eps: 0.391792\n",
            " 51806/100000: episode: 72, duration: 7.320s, episode steps: 677, steps per second:  92, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 36.163224, mean_q: 162.174177, mean_eps: 0.382396\n",
            " 52149/100000: episode: 73, duration: 3.745s, episode steps: 343, steps per second:  92, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 34.663098, mean_q: 162.471551, mean_eps: 0.376276\n",
            " 53409/100000: episode: 74, duration: 13.748s, episode steps: 1260, steps per second:  92, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 38.424360, mean_q: 163.227248, mean_eps: 0.366658\n",
            " 53965/100000: episode: 75, duration: 6.050s, episode steps: 556, steps per second:  92, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 30.775922, mean_q: 163.323542, mean_eps: 0.355762\n",
            " 54895/100000: episode: 76, duration: 10.044s, episode steps: 930, steps per second:  93, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 35.354241, mean_q: 163.313742, mean_eps: 0.346846\n",
            " 55484/100000: episode: 77, duration: 6.517s, episode steps: 589, steps per second:  90, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 35.582344, mean_q: 163.122091, mean_eps: 0.337732\n",
            " 55963/100000: episode: 78, duration: 5.243s, episode steps: 479, steps per second:  91, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 37.664553, mean_q: 162.821943, mean_eps: 0.331324\n",
            " 57142/100000: episode: 79, duration: 12.952s, episode steps: 1179, steps per second:  91, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 36.315439, mean_q: 163.385804, mean_eps: 0.321376\n",
            " 57469/100000: episode: 80, duration: 3.634s, episode steps: 327, steps per second:  90, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.948 [0.000, 5.000],  loss: 28.244932, mean_q: 163.290755, mean_eps: 0.312340\n",
            " 58156/100000: episode: 81, duration: 7.466s, episode steps: 687, steps per second:  92, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 39.911090, mean_q: 163.979745, mean_eps: 0.306256\n",
            " 59049/100000: episode: 82, duration: 9.822s, episode steps: 893, steps per second:  91, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 29.994015, mean_q: 163.866945, mean_eps: 0.296776\n",
            " 59493/100000: episode: 83, duration: 4.829s, episode steps: 444, steps per second:  92, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.984 [0.000, 5.000],  loss: 37.782161, mean_q: 164.116562, mean_eps: 0.288754\n",
            " 60171/100000: episode: 84, duration: 7.951s, episode steps: 678, steps per second:  85, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 31.354521, mean_q: 164.359907, mean_eps: 0.282022\n",
            " 60666/100000: episode: 85, duration: 5.495s, episode steps: 495, steps per second:  90, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 24.096064, mean_q: 164.896147, mean_eps: 0.274984\n",
            " 61859/100000: episode: 86, duration: 13.187s, episode steps: 1193, steps per second:  90, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 30.839942, mean_q: 165.178751, mean_eps: 0.264856\n",
            " 62748/100000: episode: 87, duration: 9.703s, episode steps: 889, steps per second:  92, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 29.809139, mean_q: 165.064183, mean_eps: 0.252364\n",
            " 63913/100000: episode: 88, duration: 12.715s, episode steps: 1165, steps per second:  92, episode reward: 19.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 29.300267, mean_q: 165.408638, mean_eps: 0.240040\n",
            " 64569/100000: episode: 89, duration: 7.296s, episode steps: 656, steps per second:  90, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.155 [0.000, 5.000],  loss: 34.901631, mean_q: 165.969020, mean_eps: 0.229114\n",
            " 65091/100000: episode: 90, duration: 5.737s, episode steps: 522, steps per second:  91, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.182 [0.000, 5.000],  loss: 27.529564, mean_q: 165.699677, mean_eps: 0.222046\n",
            " 65505/100000: episode: 91, duration: 4.543s, episode steps: 414, steps per second:  91, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 30.810483, mean_q: 166.267769, mean_eps: 0.216430\n",
            " 66418/100000: episode: 92, duration: 9.978s, episode steps: 913, steps per second:  92, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 27.243218, mean_q: 165.631836, mean_eps: 0.208468\n",
            " 67183/100000: episode: 93, duration: 8.492s, episode steps: 765, steps per second:  90, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 30.390262, mean_q: 165.913599, mean_eps: 0.198400\n",
            " 67562/100000: episode: 94, duration: 4.155s, episode steps: 379, steps per second:  91, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.050 [0.000, 5.000],  loss: 26.711737, mean_q: 166.522293, mean_eps: 0.191536\n",
            " 68985/100000: episode: 95, duration: 15.649s, episode steps: 1423, steps per second:  91, episode reward: 20.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 31.048822, mean_q: 166.112201, mean_eps: 0.180724\n",
            " 69489/100000: episode: 96, duration: 5.515s, episode steps: 504, steps per second:  91, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 40.179366, mean_q: 166.076271, mean_eps: 0.169162\n",
            " 70109/100000: episode: 97, duration: 6.946s, episode steps: 620, steps per second:  89, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 30.338330, mean_q: 166.287268, mean_eps: 0.162418\n",
            " 70520/100000: episode: 98, duration: 4.513s, episode steps: 411, steps per second:  91, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.769 [0.000, 5.000],  loss: 26.348495, mean_q: 165.846722, mean_eps: 0.156232\n",
            " 71480/100000: episode: 99, duration: 10.519s, episode steps: 960, steps per second:  91, episode reward:  6.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 28.772090, mean_q: 166.260726, mean_eps: 0.148006\n",
            " 72160/100000: episode: 100, duration: 7.570s, episode steps: 680, steps per second:  90, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 26.512452, mean_q: 166.179101, mean_eps: 0.138166\n",
            " 72960/100000: episode: 101, duration: 8.990s, episode steps: 800, steps per second:  89, episode reward:  4.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 22.683422, mean_q: 166.150679, mean_eps: 0.129286\n",
            " 73655/100000: episode: 102, duration: 7.706s, episode steps: 695, steps per second:  90, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.862 [0.000, 5.000],  loss: 28.297476, mean_q: 165.971038, mean_eps: 0.120316\n",
            " 74802/100000: episode: 103, duration: 12.624s, episode steps: 1147, steps per second:  91, episode reward: 13.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 29.672911, mean_q: 166.446124, mean_eps: 0.109264\n",
            " 75185/100000: episode: 104, duration: 4.447s, episode steps: 383, steps per second:  86, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 39.138129, mean_q: 166.391708, mean_eps: 0.100617\n",
            " 75856/100000: episode: 105, duration: 7.386s, episode steps: 671, steps per second:  91, episode reward:  5.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 24.115958, mean_q: 166.173915, mean_eps: 0.100000\n",
            " 76458/100000: episode: 106, duration: 6.653s, episode steps: 602, steps per second:  90, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 28.926442, mean_q: 166.579830, mean_eps: 0.100000\n",
            " 77102/100000: episode: 107, duration: 7.134s, episode steps: 644, steps per second:  90, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.927 [0.000, 5.000],  loss: 30.148964, mean_q: 166.231184, mean_eps: 0.100000\n",
            " 77613/100000: episode: 108, duration: 5.669s, episode steps: 511, steps per second:  90, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 31.703023, mean_q: 167.175607, mean_eps: 0.100000\n",
            " 78295/100000: episode: 109, duration: 8.399s, episode steps: 682, steps per second:  81, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 22.080704, mean_q: 166.674763, mean_eps: 0.100000\n",
            " 79261/100000: episode: 110, duration: 10.778s, episode steps: 966, steps per second:  90, episode reward:  8.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 1.879 [0.000, 5.000],  loss: 25.989790, mean_q: 166.976540, mean_eps: 0.100000\n",
            " 79861/100000: episode: 111, duration: 6.727s, episode steps: 600, steps per second:  89, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.685 [0.000, 5.000],  loss: 23.979969, mean_q: 167.354548, mean_eps: 0.100000\n",
            " 80834/100000: episode: 112, duration: 10.982s, episode steps: 973, steps per second:  89, episode reward: 11.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.741 [0.000, 5.000],  loss: 27.461330, mean_q: 167.724771, mean_eps: 0.100000\n",
            " 81456/100000: episode: 113, duration: 6.914s, episode steps: 622, steps per second:  90, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 20.552800, mean_q: 167.662517, mean_eps: 0.100000\n",
            " 82333/100000: episode: 114, duration: 9.892s, episode steps: 877, steps per second:  89, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 30.916644, mean_q: 168.125938, mean_eps: 0.100000\n",
            " 82810/100000: episode: 115, duration: 5.401s, episode steps: 477, steps per second:  88, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 1.824 [0.000, 5.000],  loss: 22.292556, mean_q: 168.502516, mean_eps: 0.100000\n",
            " 83498/100000: episode: 116, duration: 7.796s, episode steps: 688, steps per second:  88, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.776 [0.000, 5.000],  loss: 26.073926, mean_q: 168.369584, mean_eps: 0.100000\n",
            " 83850/100000: episode: 117, duration: 3.953s, episode steps: 352, steps per second:  89, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.210 [0.000, 5.000],  loss: 23.913011, mean_q: 168.539018, mean_eps: 0.100000\n",
            " 84499/100000: episode: 118, duration: 7.352s, episode steps: 649, steps per second:  88, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 23.343305, mean_q: 168.644293, mean_eps: 0.100000\n",
            " 85421/100000: episode: 119, duration: 10.310s, episode steps: 922, steps per second:  89, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 25.064859, mean_q: 168.955795, mean_eps: 0.100000\n",
            " 85956/100000: episode: 120, duration: 6.086s, episode steps: 535, steps per second:  88, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.196 [0.000, 5.000],  loss: 23.049179, mean_q: 169.029623, mean_eps: 0.100000\n",
            " 86319/100000: episode: 121, duration: 4.129s, episode steps: 363, steps per second:  88, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 1.871 [0.000, 5.000],  loss: 27.094864, mean_q: 169.262349, mean_eps: 0.100000\n",
            " 87414/100000: episode: 122, duration: 12.264s, episode steps: 1095, steps per second:  89, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 25.440697, mean_q: 168.970154, mean_eps: 0.100000\n",
            " 87903/100000: episode: 123, duration: 5.509s, episode steps: 489, steps per second:  89, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 17.641857, mean_q: 169.396048, mean_eps: 0.100000\n",
            " 88269/100000: episode: 124, duration: 4.229s, episode steps: 366, steps per second:  87, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 22.179864, mean_q: 169.514721, mean_eps: 0.100000\n",
            " 89374/100000: episode: 125, duration: 12.331s, episode steps: 1105, steps per second:  90, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 26.693202, mean_q: 169.521733, mean_eps: 0.100000\n",
            " 90076/100000: episode: 126, duration: 7.941s, episode steps: 702, steps per second:  88, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.017 [0.000, 5.000],  loss: 22.092163, mean_q: 169.508019, mean_eps: 0.100000\n",
            " 91068/100000: episode: 127, duration: 11.299s, episode steps: 992, steps per second:  88, episode reward:  6.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.070 [0.000, 5.000],  loss: 22.440360, mean_q: 168.329702, mean_eps: 0.100000\n",
            " 91563/100000: episode: 128, duration: 5.572s, episode steps: 495, steps per second:  89, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.156 [0.000, 5.000],  loss: 20.068738, mean_q: 168.369996, mean_eps: 0.100000\n",
            " 92599/100000: episode: 129, duration: 11.657s, episode steps: 1036, steps per second:  89, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.979 [0.000, 5.000],  loss: 23.515230, mean_q: 168.532341, mean_eps: 0.100000\n",
            " 93237/100000: episode: 130, duration: 7.231s, episode steps: 638, steps per second:  88, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.826 [0.000, 5.000],  loss: 24.487754, mean_q: 168.441526, mean_eps: 0.100000\n",
            " 93899/100000: episode: 131, duration: 7.649s, episode steps: 662, steps per second:  87, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.828 [0.000, 5.000],  loss: 18.554329, mean_q: 168.431221, mean_eps: 0.100000\n",
            " 94793/100000: episode: 132, duration: 10.133s, episode steps: 894, steps per second:  88, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 26.938023, mean_q: 168.583297, mean_eps: 0.100000\n",
            " 95448/100000: episode: 133, duration: 7.460s, episode steps: 655, steps per second:  88, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.011 [0.000, 5.000],  loss: 30.189126, mean_q: 168.243342, mean_eps: 0.100000\n",
            " 95993/100000: episode: 134, duration: 6.183s, episode steps: 545, steps per second:  88, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.233 [0.000, 5.000],  loss: 19.024084, mean_q: 168.630885, mean_eps: 0.100000\n",
            " 96641/100000: episode: 135, duration: 8.132s, episode steps: 648, steps per second:  80, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.873 [0.000, 5.000],  loss: 24.105928, mean_q: 168.683477, mean_eps: 0.100000\n",
            " 97262/100000: episode: 136, duration: 7.074s, episode steps: 621, steps per second:  88, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 21.951722, mean_q: 168.403789, mean_eps: 0.100000\n",
            " 97989/100000: episode: 137, duration: 8.205s, episode steps: 727, steps per second:  89, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.898 [0.000, 5.000],  loss: 22.706321, mean_q: 168.878053, mean_eps: 0.100000\n",
            " 98620/100000: episode: 138, duration: 7.258s, episode steps: 631, steps per second:  87, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 1.794 [0.000, 5.000],  loss: 25.471303, mean_q: 168.696568, mean_eps: 0.100000\n",
            " 99072/100000: episode: 139, duration: 5.284s, episode steps: 452, steps per second:  86, episode reward:  1.000, mean reward:  0.002 [ 0.000,  1.000], mean action: 2.204 [0.000, 5.000],  loss: 22.850239, mean_q: 168.556693, mean_eps: 0.100000\n",
            " 99845/100000: episode: 140, duration: 8.809s, episode steps: 773, steps per second:  88, episode reward:  5.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 25.595614, mean_q: 168.973378, mean_eps: 0.100000\n",
            "done, took 1007.741 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = ram_dqn.test(envram, nb_episodes=100, visualize=False)\n",
        "np.mean(scores.history[\"episode_reward\"])"
      ],
      "metadata": {
        "id": "rTQ_pbQLRtc0",
        "outputId": "eea69d20-db88-4037-d551-21c676e985c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing for 100 episodes ...\n",
            "Episode 1: reward: 0.000, steps: 884\n",
            "Episode 2: reward: 0.000, steps: 527\n",
            "Episode 3: reward: 0.000, steps: 731\n",
            "Episode 4: reward: 0.000, steps: 1055\n",
            "Episode 5: reward: 1.000, steps: 1115\n",
            "Episode 6: reward: 0.000, steps: 682\n",
            "Episode 7: reward: 0.000, steps: 786\n",
            "Episode 8: reward: 0.000, steps: 490\n",
            "Episode 9: reward: 0.000, steps: 623\n",
            "Episode 10: reward: 0.000, steps: 1088\n",
            "Episode 11: reward: 0.000, steps: 684\n",
            "Episode 12: reward: 0.000, steps: 476\n",
            "Episode 13: reward: 0.000, steps: 640\n",
            "Episode 14: reward: 0.000, steps: 494\n",
            "Episode 15: reward: 0.000, steps: 650\n",
            "Episode 16: reward: 0.000, steps: 477\n",
            "Episode 17: reward: 0.000, steps: 390\n",
            "Episode 18: reward: 0.000, steps: 500\n",
            "Episode 19: reward: 0.000, steps: 670\n",
            "Episode 20: reward: 0.000, steps: 619\n",
            "Episode 21: reward: 0.000, steps: 675\n",
            "Episode 22: reward: 0.000, steps: 909\n",
            "Episode 23: reward: 0.000, steps: 946\n",
            "Episode 24: reward: 0.000, steps: 479\n",
            "Episode 25: reward: 0.000, steps: 777\n",
            "Episode 26: reward: 0.000, steps: 591\n",
            "Episode 27: reward: 0.000, steps: 478\n",
            "Episode 28: reward: 0.000, steps: 503\n",
            "Episode 29: reward: 1.000, steps: 1079\n",
            "Episode 30: reward: 0.000, steps: 922\n",
            "Episode 31: reward: 0.000, steps: 678\n",
            "Episode 32: reward: 0.000, steps: 647\n",
            "Episode 33: reward: 3.000, steps: 396\n",
            "Episode 34: reward: 0.000, steps: 509\n",
            "Episode 35: reward: 0.000, steps: 651\n",
            "Episode 36: reward: 0.000, steps: 790\n",
            "Episode 37: reward: 0.000, steps: 1211\n",
            "Episode 38: reward: 0.000, steps: 621\n",
            "Episode 39: reward: 0.000, steps: 1025\n",
            "Episode 40: reward: 0.000, steps: 528\n",
            "Episode 41: reward: 0.000, steps: 1087\n",
            "Episode 42: reward: 0.000, steps: 657\n",
            "Episode 43: reward: 0.000, steps: 672\n",
            "Episode 44: reward: 0.000, steps: 885\n",
            "Episode 45: reward: 0.000, steps: 495\n",
            "Episode 46: reward: 0.000, steps: 680\n",
            "Episode 47: reward: 0.000, steps: 922\n",
            "Episode 48: reward: 0.000, steps: 930\n",
            "Episode 49: reward: 0.000, steps: 650\n",
            "Episode 50: reward: 0.000, steps: 618\n",
            "Episode 51: reward: 0.000, steps: 788\n",
            "Episode 52: reward: 0.000, steps: 647\n",
            "Episode 53: reward: 0.000, steps: 847\n",
            "Episode 54: reward: 0.000, steps: 511\n",
            "Episode 55: reward: 0.000, steps: 699\n",
            "Episode 56: reward: 0.000, steps: 607\n",
            "Episode 57: reward: 0.000, steps: 880\n",
            "Episode 58: reward: 0.000, steps: 613\n",
            "Episode 59: reward: 0.000, steps: 474\n",
            "Episode 60: reward: 0.000, steps: 516\n",
            "Episode 61: reward: 0.000, steps: 512\n",
            "Episode 62: reward: 0.000, steps: 897\n",
            "Episode 63: reward: 0.000, steps: 1018\n",
            "Episode 64: reward: 0.000, steps: 1127\n",
            "Episode 65: reward: 0.000, steps: 954\n",
            "Episode 66: reward: 0.000, steps: 637\n",
            "Episode 67: reward: 0.000, steps: 901\n",
            "Episode 68: reward: 0.000, steps: 1014\n",
            "Episode 69: reward: 0.000, steps: 695\n",
            "Episode 70: reward: 0.000, steps: 885\n",
            "Episode 71: reward: 0.000, steps: 477\n",
            "Episode 72: reward: 0.000, steps: 656\n",
            "Episode 73: reward: 0.000, steps: 1076\n",
            "Episode 74: reward: 0.000, steps: 646\n",
            "Episode 75: reward: 0.000, steps: 519\n",
            "Episode 76: reward: 0.000, steps: 1094\n",
            "Episode 77: reward: 0.000, steps: 606\n",
            "Episode 78: reward: 0.000, steps: 634\n",
            "Episode 79: reward: 0.000, steps: 762\n",
            "Episode 80: reward: 0.000, steps: 840\n",
            "Episode 81: reward: 0.000, steps: 1229\n",
            "Episode 82: reward: 0.000, steps: 918\n",
            "Episode 83: reward: 0.000, steps: 495\n",
            "Episode 84: reward: 0.000, steps: 943\n",
            "Episode 85: reward: 0.000, steps: 517\n",
            "Episode 86: reward: 0.000, steps: 621\n",
            "Episode 87: reward: 0.000, steps: 690\n",
            "Episode 88: reward: 0.000, steps: 733\n",
            "Episode 89: reward: 0.000, steps: 894\n",
            "Episode 90: reward: 0.000, steps: 498\n",
            "Episode 91: reward: 0.000, steps: 860\n",
            "Episode 92: reward: 0.000, steps: 693\n",
            "Episode 93: reward: 0.000, steps: 657\n",
            "Episode 94: reward: 0.000, steps: 623\n",
            "Episode 95: reward: 0.000, steps: 464\n",
            "Episode 96: reward: 0.000, steps: 1084\n",
            "Episode 97: reward: 0.000, steps: 1156\n",
            "Episode 98: reward: 0.000, steps: 949\n",
            "Episode 99: reward: 0.000, steps: 652\n",
            "Episode 100: reward: 0.000, steps: 395\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ram_dqn.save_weights(\"ramdqn.h5f\", overwrite=True)"
      ],
      "metadata": {
        "id": "zWIrLGEz0axU"
      },
      "execution_count": 53,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "atari.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}