{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2wsuq_uaCwE"
      },
      "source": [
        "# Курсова работа по \"Обучение по метода поощрение/наказание\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "на Александър Игнатов, Ф№0MI3400082, 16.06.2022г."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Тема: „Приложение на метода Deep Q-Learning (DQN) за играта Space Invaders за ретро игрови конзоли Atari 2600.“"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Избран обект от Gym\n",
        "\n",
        "Обект на обучението е играта Space Invaders за Atari 2600.\n",
        "\n",
        "![game](https://www.gymlibrary.ml/_images/space_invaders.gif)\n",
        "\n",
        "Играчът контролира наземна ракета, която придвижва наляво или надясно, и от която може да стреля нагоре. Точки печели, когато унищожава противникови ракети и губи играта, когато те достигнат земята или го унищожат с изстрел. Целта е максимално натрупване на точки в рамките на три живота.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gym ни предоставя модел на играта с три различни възможни начина на наблюдение на състоянието и в три различни версии (v0, v4, v5). Използвана от проекта версия е v4, като са проведени експерименти с два различни типа пространство на състоянието (наблюдение на пикселите на екрана и наблюдение на RAM паметта на играта) с цел сравнение на двата подхода."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "При варианта с наблюдение на пикселите, пространството от състояния е тензор с размерности 210x160x3 (височина х ширина х цвят):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(210, 160, 3)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env = gym.make(\"SpaceInvaders-v4\")\n",
        "env.observation_space.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "При варианта с наблюдение на RAM паметта, пространството от състояния е вектор с дължина 128, понеже играта има само 128 байта в RAM паметта си:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "envram = gym.make(\"SpaceInvaders-v4\", obs_type=\"ram\")\n",
        "envram.observation_space.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Възможните действия в играта на всеки кадър са 6 на брой:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEd_Sh4nZfQU",
        "outputId": "75386dc0-948c-4e6d-e406-671cc383f94c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.unwrapped.get_action_meanings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "При избор на случайно действие на всяка стъпка се постига резултат от около 150 точки в играта (долният експеримент изчислява средноаритметичния reward при 100 епизода при случайно избиране на действие на всяка стъпка):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvgmfMu-Z3HG",
        "outputId": "73ee0323-520b-4089-c102-f2f346f4d751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 1: Reward == 540.0\n",
            "Episode 2: Reward == 180.0\n",
            "Episode 3: Reward == 35.0\n",
            "Episode 4: Reward == 110.0\n",
            "Episode 5: Reward == 395.0\n",
            "Episode 6: Reward == 60.0\n",
            "Episode 7: Reward == 85.0\n",
            "Episode 8: Reward == 30.0\n",
            "Episode 9: Reward == 135.0\n",
            "Episode 10: Reward == 65.0\n",
            "Episode 11: Reward == 185.0\n",
            "Episode 12: Reward == 180.0\n",
            "Episode 13: Reward == 180.0\n",
            "Episode 14: Reward == 50.0\n",
            "Episode 15: Reward == 185.0\n",
            "Episode 16: Reward == 360.0\n",
            "Episode 17: Reward == 135.0\n",
            "Episode 18: Reward == 210.0\n",
            "Episode 19: Reward == 110.0\n",
            "Episode 20: Reward == 155.0\n",
            "Episode 21: Reward == 110.0\n",
            "Episode 22: Reward == 90.0\n",
            "Episode 23: Reward == 125.0\n",
            "Episode 24: Reward == 115.0\n",
            "Episode 25: Reward == 50.0\n",
            "Episode 26: Reward == 210.0\n",
            "Episode 27: Reward == 90.0\n",
            "Episode 28: Reward == 430.0\n",
            "Episode 29: Reward == 120.0\n",
            "Episode 30: Reward == 105.0\n",
            "Episode 31: Reward == 110.0\n",
            "Episode 32: Reward == 155.0\n",
            "Episode 33: Reward == 170.0\n",
            "Episode 34: Reward == 105.0\n",
            "Episode 35: Reward == 210.0\n",
            "Episode 36: Reward == 75.0\n",
            "Episode 37: Reward == 110.0\n",
            "Episode 38: Reward == 180.0\n",
            "Episode 39: Reward == 120.0\n",
            "Episode 40: Reward == 75.0\n",
            "Episode 41: Reward == 270.0\n",
            "Episode 42: Reward == 210.0\n",
            "Episode 43: Reward == 110.0\n",
            "Episode 44: Reward == 90.0\n",
            "Episode 45: Reward == 65.0\n",
            "Episode 46: Reward == 210.0\n",
            "Episode 47: Reward == 255.0\n",
            "Episode 48: Reward == 135.0\n",
            "Episode 49: Reward == 180.0\n",
            "Episode 50: Reward == 155.0\n",
            "Episode 51: Reward == 210.0\n",
            "Episode 52: Reward == 220.0\n",
            "Episode 53: Reward == 355.0\n",
            "Episode 54: Reward == 90.0\n",
            "Episode 55: Reward == 225.0\n",
            "Episode 56: Reward == 55.0\n",
            "Episode 57: Reward == 120.0\n",
            "Episode 58: Reward == 195.0\n",
            "Episode 59: Reward == 105.0\n",
            "Episode 60: Reward == 125.0\n",
            "Episode 61: Reward == 80.0\n",
            "Episode 62: Reward == 50.0\n",
            "Episode 63: Reward == 30.0\n",
            "Episode 64: Reward == 145.0\n",
            "Episode 65: Reward == 260.0\n",
            "Episode 66: Reward == 135.0\n",
            "Episode 67: Reward == 105.0\n",
            "Episode 68: Reward == 120.0\n",
            "Episode 69: Reward == 105.0\n",
            "Episode 70: Reward == 75.0\n",
            "Episode 71: Reward == 65.0\n",
            "Episode 72: Reward == 155.0\n",
            "Episode 73: Reward == 65.0\n",
            "Episode 74: Reward == 80.0\n",
            "Episode 75: Reward == 365.0\n",
            "Episode 76: Reward == 90.0\n",
            "Episode 77: Reward == 120.0\n",
            "Episode 78: Reward == 145.0\n",
            "Episode 79: Reward == 30.0\n",
            "Episode 80: Reward == 410.0\n",
            "Episode 81: Reward == 335.0\n",
            "Episode 82: Reward == 20.0\n",
            "Episode 83: Reward == 135.0\n",
            "Episode 84: Reward == 210.0\n",
            "Episode 85: Reward == 180.0\n",
            "Episode 86: Reward == 10.0\n",
            "Episode 87: Reward == 110.0\n",
            "Episode 88: Reward == 110.0\n",
            "Episode 89: Reward == 425.0\n",
            "Episode 90: Reward == 110.0\n",
            "Episode 91: Reward == 120.0\n",
            "Episode 92: Reward == 95.0\n",
            "Episode 93: Reward == 80.0\n",
            "Episode 94: Reward == 265.0\n",
            "Episode 95: Reward == 105.0\n",
            "Episode 96: Reward == 195.0\n",
            "Episode 97: Reward == 150.0\n",
            "Episode 98: Reward == 435.0\n",
            "Episode 99: Reward == 180.0\n",
            "Episode 100: Reward == 155.0\n",
            "Average reward: 155.75\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "EPISODES = 100\n",
        "scores = []\n",
        "\n",
        "for episode in range(1, EPISODES + 1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0 \n",
        "    \n",
        "    while not done:\n",
        "        action = random.choice(range(env.action_space.n))\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "    \n",
        "    scores.append(score)\n",
        "    print(f\"Episode {episode}: Reward == {score}\")\n",
        "\n",
        "avg = np.mean(scores)\n",
        "print(f\"Average reward: {avg}\")\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTzMWfxFdDrS"
      },
      "source": [
        "## Избран метод за обучение\n",
        "\n",
        "Избраният метод за обучение е Deep Q-Learning (DQN) с Experience Replay и $\\epsilon$\u000f-greedy стратегия (https://arxiv.org/pdf/1312.5602.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Псевдокод на алгоритъма:\n",
        "1. Инициализация на replay memory $D$ с капацитет $N$\n",
        "2. Инициализация на Q-функцията със случайни тегла\n",
        "3. За $episode = 1...M$, направи:\n",
        "    * Инициализирай редица $ s_1 = {x_1} $\n",
        "    * За $ t = 1... T $, направи:\n",
        "        * С вероятност $\\epsilon$ избери случайно действие $a_t$, в противен случай избери $ a_t = argmax_a Q^*(s_t, a; \\theta) $\n",
        "        * Изпълни действието в емулатора и наблюдавай награда $r_t$ и състояние $x_{t+1}$\n",
        "        * Запази прехода $ (s_t, a_t, r_t, s_{t+1}) $ в $ D $\n",
        "        * Вземи произволно малко количество от преходи $ (s_j , a_j , r_j , s_{j+1}) $ от $ D $\n",
        "        * Присвои $$ y_j =\n",
        "            \\begin{cases}\n",
        "            r_j & \\text{за терминален } s_{j+1} \\\\\n",
        "            r_j + \\gamma \\max_{a'} Q^*(s_{j+1}, a'; \\theta) & \\text{за нетерминален } s_{j+1}\n",
        "            \\end{cases} $$\n",
        "        * Направи градиентно спускане по $ (y_j − Q(s_j, a_j; \\theta))^2 $:\n",
        "$$ \\nabla_{\\theta_i}L_i(\\theta_i) = E_{s,a \\sim p(·); s' \\sim \\epsilon} [(r + \\gamma\\max_a'Q(s', a'; \\theta_{i-1}) - Q(s, a, \\theta_i))\\nabla_{\\theta_i}Q(s, a; \\theta_i)] $$\n",
        "$$ L_i(\\theta_i) = E_{s,a \\sim p(·)}[(y_i - Q(s, a; \\theta_i))^2] $$\n",
        "$$% Q^*(s, a) = E_{s' \\sim \\epsilon}[r + \\gamma\\max_a'Q^*(s', a') \\mid s, a] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Реализация и ескперименти"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFF_4nRjdJOy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Convolution2D, Resizing, Rescaling, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.image import rgb_to_grayscale\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.utils import register_keras_serializable\n",
        "from rl.agents import DQNAgent\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0. Функция за създаване на агента\n",
        "\n",
        "Хиперпараметрите, които са използвани, са:\n",
        "* Капацитет на experience replay паметта: 1 000 000\n",
        "* Намаляне на $\\epsilon$: от 1.0 до 0.1 в хода на 10 000 стъпки\n",
        "* Брой стъпки преди започване на обучението: 1 000\n",
        "* $\\gamma = 0.99$\n",
        "* Размер на batch: 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "un0NM5Cee05L"
      },
      "outputs": [],
      "source": [
        "def build_agent(model, actions, window_size):\n",
        "    policy = LinearAnnealedPolicy(\n",
        "        EpsGreedyQPolicy(), \n",
        "        attr='eps', \n",
        "        value_max=1.0, \n",
        "        value_min=0.1, \n",
        "        value_test=0.2, \n",
        "        nb_steps=10000\n",
        "    )\n",
        "    memory = SequentialMemory(\n",
        "        limit=1000000, \n",
        "        window_length=window_size\n",
        "    )\n",
        "    dqn = DQNAgent(\n",
        "        model=model, \n",
        "        memory=memory, \n",
        "        policy=policy,\n",
        "        enable_dueling_network=True, \n",
        "        dueling_type='avg', \n",
        "        nb_actions=actions, \n",
        "        nb_steps_warmup=1000\n",
        "    )\n",
        "    return dqn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Наблюдение на пикселното изображение на играта\n",
        "\n",
        "Стандартно за тази цел е използването на конволюционни слоеве в невронната мрежа с няколко филтри. Бе експериментирано с различни конфигурации на мрежата, като при всички тях времето за обучение бе изключително бавно. Поради това изображението с размер 210х160 се намаля наполовина по двете измерения и конвертира в черно-бяло такова, което намаля размерността на входящия тензор до 105х80х1, което значително намалява параметрите за обучение, но не даде забележим резултат за времето за обучение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YWjHyJ6T3Bof"
      },
      "outputs": [],
      "source": [
        "@register_keras_serializable(\"atari\")\n",
        "class GrayscaleLayer(Layer):\n",
        "  def call(self, input):\n",
        "    return rgb_to_grayscale(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_QXMNUladM_T"
      },
      "outputs": [],
      "source": [
        "def build_model(window_size, height, width, channels, actions):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(window_size, height, width, channels)))\n",
        "    model.add(Reshape((window_size * height, width, channels), name=\"reshape_stack\"))\n",
        "    model.add(GrayscaleLayer(name=\"grayscale\"))\n",
        "    model.add(Resizing((window_size * height) // 2, width // 2, name=\"resize_half\"))\n",
        "    model.add(Rescaling(1./255, name=\"normalize\")) # normalize to [0, 1]\n",
        "    model.add(Reshape((window_size, height // 2, width // 2, 1), name=\"reshape_unstack\"))\n",
        "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', name=\"conv1\"))\n",
        "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu', name=\"conv2\"))\n",
        "    model.add(Convolution2D(64, (3,3), activation='relu', name=\"conv3\"))\n",
        "    model.add(Flatten(name=\"flatten\"))\n",
        "    model.add(Dense(512, activation='relu', name=\"fully_connected_1\"))\n",
        "    # model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear', name=\"output\"))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6rdDUGF1dgVE"
      },
      "outputs": [],
      "source": [
        "WINDOW_SIZE = 4\n",
        "height, width, channels = env.observation_space.shape\n",
        "actions = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Използвайки `WINDOW_SIZE = 4` задаваме едновременната обработка на последните 4 състояния на играта на всяка стъпка. Това се прави с цел определяне на посоката на разитие."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ_R6xoWdolT",
        "outputId": "4e6e1311-e274-4839-d9da-2327ddc268ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"seqmodel\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 840, 160, 3)       0         \n",
            "                                                                 \n",
            " grayscale (GrayscaleLayer)  (None, 840, 160, 1)       0         \n",
            "                                                                 \n",
            " resizing (Resizing)         (None, 420, 80, 1)        0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 420, 80, 1)        0         \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 4, 105, 80, 1)     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 4, 25, 19, 32)     2080      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 4, 11, 8, 64)      32832     \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 4, 9, 6, 64)       36928     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 13824)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               7078400   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,153,318\n",
            "Trainable params: 7,153,318\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = build_model(WINDOW_SIZE, height, width, channels, actions)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Както се вижда, получената мрежа с три конволюционни слоя и един изцяло-свързан слой с 512 неврона съдържа малко над 7 милиона параметъра (тегла) за обучение, които дори и с видеокартите, предоставени от Google Collab са твърде много за да може обучението да се впише в предоставениете ми няколко часа прозорец за активност на платформата."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XCK4WyX1f1VF"
      },
      "outputs": [],
      "source": [
        "dqn = build_agent(model, actions, WINDOW_SIZE)\n",
        "dqn.compile(Adam(learning_rate=0.00025))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOP8OrohiUj6",
        "outputId": "12d7ef3f-9081-4010-f3c7-2fe14f902a02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  545/10000: episode: 1, duration: 16.783s, episode steps: 545, steps per second:  32, episode reward: 120.000, mean reward:  0.220 [ 0.000, 30.000], mean action: 2.461 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1906/10000: episode: 2, duration: 1463.794s, episode steps: 1361, steps per second:   1, episode reward: 605.000, mean reward:  0.445 [ 0.000, 200.000], mean action: 2.497 [0.000, 5.000],  loss: 8.339929, mean_q: 0.921901, mean_eps: 0.869230\n",
            " 2575/10000: episode: 3, duration: 1071.301s, episode steps: 669, steps per second:   1, episode reward: 110.000, mean reward:  0.164 [ 0.000, 25.000], mean action: 2.469 [0.000, 5.000],  loss: 14.594663, mean_q: 1.724796, mean_eps: 0.798400\n",
            " 4166/10000: episode: 4, duration: 2526.268s, episode steps: 1591, steps per second:   1, episode reward: 250.000, mean reward:  0.157 [ 0.000, 30.000], mean action: 2.456 [0.000, 5.000],  loss: 2.485551, mean_q: 0.938030, mean_eps: 0.696700\n",
            " 4569/10000: episode: 5, duration: 642.676s, episode steps: 403, steps per second:   1, episode reward: 55.000, mean reward:  0.136 [ 0.000, 20.000], mean action: 2.546 [0.000, 5.000],  loss: 0.923119, mean_q: 0.454591, mean_eps: 0.606970\n",
            " 5166/10000: episode: 6, duration: 951.596s, episode steps: 597, steps per second:   1, episode reward: 175.000, mean reward:  0.293 [ 0.000, 30.000], mean action: 2.467 [0.000, 5.000],  loss: 1.286257, mean_q: 0.594287, mean_eps: 0.561970\n",
            " 5668/10000: episode: 7, duration: 802.174s, episode steps: 502, steps per second:   1, episode reward: 130.000, mean reward:  0.259 [ 0.000, 30.000], mean action: 2.239 [0.000, 5.000],  loss: 1.842880, mean_q: 1.066780, mean_eps: 0.512515\n",
            " 6085/10000: episode: 8, duration: 664.656s, episode steps: 417, steps per second:   1, episode reward: 55.000, mean reward:  0.132 [ 0.000, 20.000], mean action: 2.252 [0.000, 5.000],  loss: 1.886560, mean_q: 1.025469, mean_eps: 0.471160\n",
            " 6693/10000: episode: 9, duration: 966.496s, episode steps: 608, steps per second:   1, episode reward: 145.000, mean reward:  0.238 [ 0.000, 30.000], mean action: 2.521 [0.000, 5.000],  loss: 1.815734, mean_q: 0.812441, mean_eps: 0.425035\n",
            " 7356/10000: episode: 10, duration: 1052.600s, episode steps: 663, steps per second:   1, episode reward: 75.000, mean reward:  0.113 [ 0.000, 25.000], mean action: 2.867 [0.000, 5.000],  loss: 1.978757, mean_q: 0.818287, mean_eps: 0.367840\n",
            " 8047/10000: episode: 11, duration: 1095.542s, episode steps: 691, steps per second:   1, episode reward: 170.000, mean reward:  0.246 [ 0.000, 25.000], mean action: 2.700 [0.000, 5.000],  loss: 1.064034, mean_q: 0.558667, mean_eps: 0.306910\n",
            " 9161/10000: episode: 12, duration: 1751.153s, episode steps: 1114, steps per second:   1, episode reward: 270.000, mean reward:  0.242 [ 0.000, 30.000], mean action: 2.581 [0.000, 5.000],  loss: 1.322051, mean_q: 0.697359, mean_eps: 0.225685\n",
            " 9677/10000: episode: 13, duration: 813.462s, episode steps: 516, steps per second:   1, episode reward: 85.000, mean reward:  0.165 [ 0.000, 20.000], mean action: 2.382 [0.000, 5.000],  loss: 1.574947, mean_q: 0.554374, mean_eps: 0.152335\n",
            "done, took 14326.813 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd0d80f7f50>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxnlZ2DdqMmO",
        "outputId": "65be622b-8feb-4b11-c899-fc4d6bef1281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 40.000, steps: 694\n",
            "Episode 2: reward: 230.000, steps: 826\n",
            "Episode 3: reward: 25.000, steps: 570\n",
            "Episode 4: reward: 60.000, steps: 792\n",
            "Episode 5: reward: 35.000, steps: 557\n",
            "Episode 6: reward: 20.000, steps: 661\n",
            "Episode 7: reward: 225.000, steps: 967\n",
            "Episode 8: reward: 115.000, steps: 844\n",
            "Episode 9: reward: 80.000, steps: 677\n",
            "Episode 10: reward: 85.000, steps: 545\n",
            "Episode 11: reward: 75.000, steps: 672\n",
            "Episode 12: reward: 90.000, steps: 1360\n",
            "Episode 13: reward: 50.000, steps: 375\n",
            "Episode 14: reward: 105.000, steps: 1188\n",
            "Episode 15: reward: 80.000, steps: 683\n",
            "Episode 16: reward: 15.000, steps: 497\n",
            "Episode 17: reward: 340.000, steps: 988\n",
            "Episode 18: reward: 10.000, steps: 734\n",
            "Episode 19: reward: 65.000, steps: 435\n",
            "Episode 20: reward: 20.000, steps: 407\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "88.25"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = dqn.test(env, nb_episodes=20, visualize=False)\n",
        "np.mean(scores.history[\"episode_reward\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Виждаме, че 10 000 стъпки са отнели 14326.813 секунди (почти 4 часа) и дали среден резултат от 88.25 точки на епизод, който е двойно по-лош от този при играта със случайни действия."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Наблюдение на RAM паметта на играта\n",
        "\n",
        "Предимството на използването на паметта като пространство от състояния на тази ретро игра, вместо пикселното изображение, което човек вижда, е че тя е само 128 байта, което е едно доста по-лесно смилаемо число от гледна точка на машинното самообучение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8yq6U0oPk8t",
        "outputId": "02ed6f52-c6a7-4962-bbbb-a5d49c132877"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(128,)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "envram.observation_space.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Това позволява за много по-проста структура на невронна мрежа, която е по-бърза за обучение и по-бърза за изпълнение. Един или два скрити слоя от неврони са достатъчни за тази цел."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "NE0cVmGINY84"
      },
      "outputs": [],
      "source": [
        "def build_ram_model(ram_size, actions):\n",
        "    model = Sequential(name=\"ram_model\")\n",
        "    model.add(Input(shape=(1, ram_size)))\n",
        "    model.add(Flatten(name=\"flatten\"))\n",
        "    model.add(Dense(512, activation=\"relu\", name=\"fc1\"))\n",
        "    model.add(Dense(128, activation=\"relu\", name=\"fc2\"))\n",
        "    model.add(Dense(actions, activation=\"linear\", name=\"output\"))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LfVsYVgPxsY",
        "outputId": "7bf85efa-411e-451e-b2ee-c6fa3e7d68ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"ram_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 512)               66048     \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 128)               65664     \n",
            "                                                                 \n",
            " output (Dense)              (None, 6)                 774       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 132,486\n",
            "Trainable params: 132,486\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "ram_model = build_ram_model(envram.observation_space.shape[0], envram.action_space.n)\n",
        "ram_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Виждаме, че при използване на два слоя с по 512 и 128 неврона съответно, параметрите за обучение са сведени от милиони до само над сто хиляди, което е значителна оптимизация."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA3HzQueQRIZ",
        "outputId": "d3505947-b37b-4959-cfda-cd90ba2e5692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training for 10000 steps ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  664/10000: episode: 1, duration: 1.786s, episode steps: 664, steps per second: 372, episode reward: 120.000, mean reward:  0.181 [ 0.000, 30.000], mean action: 2.489 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1166/10000: episode: 2, duration: 3.990s, episode steps: 502, steps per second: 126, episode reward: 50.000, mean reward:  0.100 [ 0.000, 20.000], mean action: 2.504 [0.000, 5.000],  loss: 216.278364, mean_q: 10.530081, mean_eps: 0.902530\n",
            " 1622/10000: episode: 3, duration: 5.496s, episode steps: 456, steps per second:  83, episode reward: 80.000, mean reward:  0.175 [ 0.000, 25.000], mean action: 2.575 [0.000, 5.000],  loss: 69.111621, mean_q: 5.582955, mean_eps: 0.874585\n",
            " 2099/10000: episode: 4, duration: 5.759s, episode steps: 477, steps per second:  83, episode reward: 60.000, mean reward:  0.126 [ 0.000, 15.000], mean action: 2.409 [0.000, 5.000],  loss: 57.648717, mean_q: 7.645651, mean_eps: 0.832600\n",
            " 2523/10000: episode: 5, duration: 5.123s, episode steps: 424, steps per second:  83, episode reward: 55.000, mean reward:  0.130 [ 0.000, 20.000], mean action: 2.611 [0.000, 5.000],  loss: 62.925592, mean_q: 7.794881, mean_eps: 0.792055\n",
            " 2985/10000: episode: 6, duration: 5.675s, episode steps: 462, steps per second:  81, episode reward: 120.000, mean reward:  0.260 [ 0.000, 30.000], mean action: 2.517 [0.000, 5.000],  loss: 60.988840, mean_q: 7.149000, mean_eps: 0.752185\n",
            " 4000/10000: episode: 7, duration: 12.643s, episode steps: 1015, steps per second:  80, episode reward: 345.000, mean reward:  0.340 [ 0.000, 30.000], mean action: 2.564 [0.000, 5.000],  loss: 58.054664, mean_q: 7.697210, mean_eps: 0.685720\n",
            " 4820/10000: episode: 8, duration: 9.940s, episode steps: 820, steps per second:  82, episode reward: 185.000, mean reward:  0.226 [ 0.000, 30.000], mean action: 2.674 [0.000, 5.000],  loss: 60.743517, mean_q: 8.666166, mean_eps: 0.603145\n",
            " 5711/10000: episode: 9, duration: 10.944s, episode steps: 891, steps per second:  81, episode reward: 95.000, mean reward:  0.107 [ 0.000, 25.000], mean action: 2.575 [0.000, 5.000],  loss: 60.203612, mean_q: 9.645707, mean_eps: 0.526150\n",
            " 6405/10000: episode: 10, duration: 8.889s, episode steps: 694, steps per second:  78, episode reward: 175.000, mean reward:  0.252 [ 0.000, 30.000], mean action: 2.667 [0.000, 5.000],  loss: 55.233722, mean_q: 8.823572, mean_eps: 0.454825\n",
            " 7057/10000: episode: 11, duration: 7.648s, episode steps: 652, steps per second:  85, episode reward: 205.000, mean reward:  0.314 [ 0.000, 30.000], mean action: 2.632 [0.000, 5.000],  loss: 54.402547, mean_q: 7.775405, mean_eps: 0.394255\n",
            " 8057/10000: episode: 12, duration: 11.823s, episode steps: 1000, steps per second:  85, episode reward: 335.000, mean reward:  0.335 [ 0.000, 30.000], mean action: 2.765 [0.000, 5.000],  loss: 56.230327, mean_q: 7.778934, mean_eps: 0.319915\n",
            " 8710/10000: episode: 13, duration: 8.155s, episode steps: 653, steps per second:  80, episode reward: 190.000, mean reward:  0.291 [ 0.000, 30.000], mean action: 2.789 [0.000, 5.000],  loss: 53.541651, mean_q: 8.630299, mean_eps: 0.245530\n",
            " 9150/10000: episode: 14, duration: 5.499s, episode steps: 440, steps per second:  80, episode reward: 65.000, mean reward:  0.148 [ 0.000, 15.000], mean action: 2.311 [0.000, 5.000],  loss: 52.831548, mean_q: 8.616217, mean_eps: 0.196345\n",
            " 9906/10000: episode: 15, duration: 9.294s, episode steps: 756, steps per second:  81, episode reward: 210.000, mean reward:  0.278 [ 0.000, 30.000], mean action: 2.775 [0.000, 5.000],  loss: 52.477634, mean_q: 9.729358, mean_eps: 0.142525\n",
            "done, took 113.856 seconds\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd0d78d9950>"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ram_dqn = build_agent(ram_model, envram.action_space.n, 1)\n",
        "ram_dqn.compile(Adam(learning_rate=0.0002))\n",
        "ram_dqn.fit(envram, nb_steps=10_000, visualize=False, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTQ_pbQLRtc0",
        "outputId": "3f7cd905-8cdf-4a25-bf28-abe79fb5c37a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing for 20 episodes ...\n",
            "Episode 1: reward: 225.000, steps: 898\n",
            "Episode 2: reward: 485.000, steps: 1027\n",
            "Episode 3: reward: 380.000, steps: 961\n",
            "Episode 4: reward: 80.000, steps: 393\n",
            "Episode 5: reward: 210.000, steps: 670\n",
            "Episode 6: reward: 135.000, steps: 525\n",
            "Episode 7: reward: 230.000, steps: 939\n",
            "Episode 8: reward: 220.000, steps: 639\n",
            "Episode 9: reward: 355.000, steps: 916\n",
            "Episode 10: reward: 400.000, steps: 1018\n",
            "Episode 11: reward: 125.000, steps: 637\n",
            "Episode 12: reward: 155.000, steps: 582\n",
            "Episode 13: reward: 110.000, steps: 468\n",
            "Episode 14: reward: 415.000, steps: 1119\n",
            "Episode 15: reward: 110.000, steps: 563\n",
            "Episode 16: reward: 80.000, steps: 528\n",
            "Episode 17: reward: 135.000, steps: 468\n",
            "Episode 18: reward: 120.000, steps: 538\n",
            "Episode 19: reward: 275.000, steps: 788\n",
            "Episode 20: reward: 355.000, steps: 1350\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "230.0"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = ram_dqn.test(envram, nb_episodes=20, visualize=False)\n",
        "np.mean(scores.history[\"episode_reward\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "В този случай след само 10 000 стъпки агентът се е научил да играе по начин, с който бележи среден резултат от 230 точки на епизод, което е по-добро от 150-те точки при случаен избор на действие."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "atari.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c4ca5fb62c4de4ad8975f8d11547f3039fc9d01585eebbf720b5eb5352efe545"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
